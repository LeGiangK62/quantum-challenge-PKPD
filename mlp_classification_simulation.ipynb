{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9599ed50-2698-4f4d-9e5e-48f7377b0002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected columns: {'ID': 'ID', 'TIME': 'TIME', 'DV': 'DV', 'EVID': 'EVID', 'AMT': 'AMT', 'BW': 'BW', 'COMED': 'COMED'}\n",
      "PD rows: 1200 Dose rows: 756\n",
      "[ID split by fixed dose groups] (70/15/15)\n",
      " train IDs: 24 | valid IDs: 6 | test IDs: 6\n",
      " 1mg -> train/valid/test: 8 2 2\n",
      " 3mg -> train/valid/test: 8 2 2\n",
      "10mg -> train/valid/test: 8 2 2\n",
      "#samples -> train: 600 | valid: 150 | test: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ep 001] lr=0.05000 acc=0.787 f1=0.000 prec=0.000 rec=0.000 roc_auc=0.701 pr_auc=0.462\n",
      "[ep 002] lr=0.05000 acc=0.787 f1=0.000 prec=0.000 rec=0.000 roc_auc=0.583 pr_auc=0.457\n",
      "[ep 003] lr=0.05000 acc=0.787 f1=0.000 prec=0.000 rec=0.000 roc_auc=0.547 pr_auc=0.420\n",
      "[ep 010] lr=0.05000 acc=0.787 f1=0.000 prec=0.000 rec=0.000 roc_auc=0.730 pr_auc=0.554\n",
      "[ep 020] lr=0.05000 acc=0.913 f1=0.772 prec=0.880 rec=0.688 roc_auc=0.908 pr_auc=0.841\n",
      "[ep 030] lr=0.05000 acc=0.940 f1=0.836 prec=1.000 rec=0.719 roc_auc=0.966 pr_auc=0.924\n",
      "[ep 040] lr=0.05000 acc=0.953 f1=0.881 prec=0.963 rec=0.812 roc_auc=0.976 pr_auc=0.941\n",
      "[ep 050] lr=0.02500 acc=0.947 f1=0.857 prec=1.000 rec=0.750 roc_auc=0.988 pr_auc=0.963\n",
      "EarlyStopping at epoch 56 (no F1 improvement for 12 epochs).\n",
      "best(valid @thr=0.5): {'epoch': 44, 'acc': 0.96, 'prec': 1.0, 'rec': 0.8125, 'f1': 0.896551724137931, 'roc_auc': np.float64(0.9883474576271187), 'pr_auc': np.float64(0.9648562601644852), 'tn': np.int64(118), 'fp': np.int64(0), 'fn': np.int64(6), 'tp': np.int64(26)}\n",
      "tuned threshold on valid: thr=0.480 | F1=0.8966 (prec=1.0000, rec=0.8125)\n",
      "\n",
      "Validation best @thr=0.5: {'epoch': 44, 'acc': 0.96, 'prec': 1.0, 'rec': 0.8125, 'f1': 0.896551724137931, 'roc_auc': np.float64(0.9883474576271187), 'pr_auc': np.float64(0.9648562601644852), 'tn': np.int64(118), 'fp': np.int64(0), 'fn': np.int64(6), 'tp': np.int64(26)}\n",
      "Validated tuned decision threshold: 0.480\n",
      "\n",
      "=== Dose recommendations summary (thr tuned) ===\n",
      "           scenario target  once-daily (mg)  once-weekly (mg)\n",
      "Base (Phase 1-like)    90%              5.5              55.0\n",
      "       BW 70–140 kg    90%              9.0              95.0\n",
      "   No COMED allowed    90%              6.0              60.0\n",
      "Base (Phase 1-like)    75%              4.0              40.0\n",
      "\n",
      "=== Final TEST metrics (unseen IDs, tuned thr) ===\n",
      "     acc: 0.9267\n",
      "    prec: 0.8095\n",
      "     rec: 0.9189\n",
      "      f1: 0.8608\n",
      " roc_auc: 0.9725\n",
      "  pr_auc: 0.9089\n",
      "      tn: 105\n",
      "      fp: 8\n",
      "      fn: 3\n",
      "      tp: 34\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# PD-only 분류 + 정상상태 용량탐색 (MLP 전용)\n",
    "# - 데이터: EstData.csv (PD: ng/mL), 임계 3.3 ng/mL\n",
    "# - 모델: MLP (레이어/히든/드롭아웃)\n",
    "# - 분할: 플라시보(0 mg, ID 1–12) 제외, 1/3/10 mg 고정 그룹별 70/15/15 ID 단위\n",
    "# - 추가: 베스트 스냅샷 복원, EarlyStopping, LR 스케줄러, Weight Decay,\n",
    "#         검증 F1 최대 임계값 튜닝 → 테스트/시뮬에 일관 적용\n",
    "# ===========================\n",
    "import os, math, warnings, numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix\n",
    ")\n",
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "# -------- 설정 --------\n",
    "CSV = \"EstData.csv\"       # 필요 시 절대경로로 교체\n",
    "PD_THRESHOLD = 3.3\n",
    "\n",
    "# 학습/최적화 설정\n",
    "EPOCHS = 120\n",
    "LR = 5e-2\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 12               # EarlyStopping 인내 에폭\n",
    "SCHED_FACTOR = 0.5          # ReduceLROnPlateau 감쇠 비율\n",
    "SCHED_PATIENCE = 5          # 스케줄러 인내 에폭(지표 기준)\n",
    "CLIP_NORM = 1.0             # 그라디언트 클리핑 (안쓰려면 None)\n",
    "\n",
    "# 시뮬레이션/검색 설정\n",
    "N_SUBJ = 300\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# MLP 하이퍼파라미터\n",
    "MLP_HIDDEN = 36\n",
    "MLP_LAYERS = 2\n",
    "MLP_DROPOUT = 0.1\n",
    "\n",
    "# 재현성(완전 결정적 보장은 환경 의존. CUDA에서 완전 결정적 필요시 CUBLAS_WORKSPACE_CONFIG 설정 필요)\n",
    "def set_global_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # 완전 결정적이 필요하면 환경변수 설정 필요(CuBLAS). 여기서는 사용하지 않음.\n",
    "\n",
    "set_global_seed(42)\n",
    "assert os.path.exists(CSV), f\"CSV not found at {CSV}\"\n",
    "\n",
    "# -------- 유틸: 열 이름 추론 --------\n",
    "def _find_col_like(df, name_opts):\n",
    "    low = {c.lower(): c for c in df.columns}\n",
    "    for n in name_opts:\n",
    "        if n in low: return low[n]\n",
    "    return None\n",
    "\n",
    "# -------- 데이터 로딩/전처리 --------\n",
    "df = pd.read_csv(CSV)\n",
    "\n",
    "col_ID   = _find_col_like(df, [\"id\"])\n",
    "col_TIME = _find_col_like(df, [\"time\"])\n",
    "col_DVID = _find_col_like(df, [\"dvid\"])\n",
    "col_DV   = _find_col_like(df, [\"dv\",\"pd\",\"value\"])\n",
    "col_EVID = _find_col_like(df, [\"evid\"])\n",
    "col_AMT  = _find_col_like(df, [\"amt\",\"dose\",\"dosen\",\"doses\"])\n",
    "col_BW   = _find_col_like(df, [\"bw\",\"weight\",\"bodyweight\"])\n",
    "col_COMED= _find_col_like(df, [\"comed\",\"conmed\",\"concom\"])\n",
    "\n",
    "need = [col_ID, col_TIME, col_DV, col_AMT]\n",
    "miss = [n for n,v in zip([\"ID\",\"TIME\",\"DV\",\"AMT/DOSE\"], need) if v is None]\n",
    "if miss:\n",
    "    warnings.warn(f\"Columns missing (minimum required): {miss}\")\n",
    "\n",
    "# PD 행 추출 (DVID==2가 있으면 그걸 사용)\n",
    "if col_DVID is not None and col_DV is not None:\n",
    "    pdf = df[df[col_DVID]==2].copy()\n",
    "else:\n",
    "    pdf = df.copy()\n",
    "\n",
    "# 투약 이벤트 (EVID==1 우선, 아니면 AMT/DOSE notna)\n",
    "if col_EVID is not None:\n",
    "    dose_df = df[df[col_EVID]==1].copy()\n",
    "else:\n",
    "    dose_df = df[df[col_AMT].notna()].copy()\n",
    "\n",
    "# 숫자 변환\n",
    "for c in [col_TIME, col_DV, col_AMT, col_BW, col_COMED]:\n",
    "    if c is not None:\n",
    "        pdf[c] = pd.to_numeric(pdf[c], errors=\"coerce\")\n",
    "        dose_df[c] = pd.to_numeric(dose_df[c], errors=\"coerce\")\n",
    "\n",
    "# 정렬/필요 열만\n",
    "keep_pd = [c for c in [col_ID,col_TIME,col_DV,col_BW,col_COMED] if c is not None]\n",
    "keep_dose = [c for c in [col_ID,col_TIME,col_AMT] if c is not None]\n",
    "pdf = pdf[keep_pd].dropna().sort_values([col_ID, col_TIME])\n",
    "dose_df = dose_df[keep_dose].dropna().sort_values([col_ID, col_TIME])\n",
    "\n",
    "print(\"Detected columns:\", dict(ID=col_ID, TIME=col_TIME, DV=col_DV, EVID=col_EVID, AMT=col_AMT, BW=col_BW, COMED=col_COMED))\n",
    "print(\"PD rows:\", len(pdf), \"Dose rows:\", len(dose_df))\n",
    "\n",
    "# -------- 데이터셋 --------\n",
    "class PDSamples(Dataset):\n",
    "    def __init__(self, pd_df: pd.DataFrame, dose_df: pd.DataFrame,\n",
    "                 col_ID: str, col_TIME: str, col_DV: str,\n",
    "                 col_BW: Optional[str], col_COMED: Optional[str], pd_threshold: float=3.3):\n",
    "        self.col_ID, self.col_TIME, self.col_DV = col_ID, col_TIME, col_DV\n",
    "        self.col_BW, self.col_COMED = col_BW, col_COMED\n",
    "        self.pd_threshold = pd_threshold\n",
    "\n",
    "        # ID별 투약 히스토리\n",
    "        d_groups = defaultdict(list)\n",
    "        for _, row in dose_df.iterrows():\n",
    "            d_groups[row[col_ID]].append((float(row[col_TIME]), float(row[col_AMT])))\n",
    "        self.dose_map = {k: (np.array([t for t,a in v], dtype=np.float32),\n",
    "                              np.array([a for t,a in v], dtype=np.float32)) for k,v in d_groups.items()}\n",
    "\n",
    "        # 샘플: 각 PD 관측 시점\n",
    "        feats = []\n",
    "        for _, row in pd_df.iterrows():\n",
    "            sid = row[col_ID]\n",
    "            if sid not in self.dose_map:\n",
    "                continue\n",
    "            t = float(row[col_TIME])\n",
    "            val = float(row[col_DV])\n",
    "            y = 1.0 if (val <= pd_threshold) else 0.0\n",
    "            bw = float(row[col_BW]) if col_BW is not None else 70.0\n",
    "            cm = float(row[col_COMED]) if col_COMED is not None else 0.0\n",
    "            feats.append((sid, t, y, bw, cm))\n",
    "        self.samples = feats\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sid, t, y, bw, cm = self.samples[idx]\n",
    "        dose_t, dose_a = self.dose_map.get(sid, (np.zeros(0, dtype=np.float32), np.zeros(0, dtype=np.float32)))\n",
    "        return {\n",
    "            \"t\": torch.tensor(t, dtype=torch.float32),\n",
    "            \"y\": torch.tensor(y, dtype=torch.float32),\n",
    "            \"bw\": torch.tensor(bw, dtype=torch.float32),\n",
    "            \"cm\": torch.tensor(cm, dtype=torch.float32),\n",
    "            \"dose_t\": torch.tensor(dose_t, dtype=torch.float32),\n",
    "            \"dose_a\": torch.tensor(dose_a, dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "def _collate(batch): return batch\n",
    "\n",
    "dataset_all = PDSamples(pdf, dose_df, col_ID, col_TIME, col_DV, col_BW, col_COMED, pd_threshold=PD_THRESHOLD)\n",
    "\n",
    "# -------- ID 단위 분할: 플라시보 제외 + 1/3/10 mg 그룹별 70/15/15 --------\n",
    "rng = 42\n",
    "placebo_ids = set(range(1, 13))     # 0 mg (제외)\n",
    "ids_1mg     = list(range(13, 25))   # 1 mg\n",
    "ids_3mg     = list(range(25, 37))   # 3 mg\n",
    "ids_10mg    = list(range(37, 49))   # 10 mg\n",
    "\n",
    "ids_in_data = set(df[col_ID].unique())\n",
    "ids_1mg  = sorted(ids_in_data.intersection(ids_1mg))\n",
    "ids_3mg  = sorted(ids_in_data.intersection(ids_3mg))\n",
    "ids_10mg = sorted(ids_in_data.intersection(ids_10mg))\n",
    "\n",
    "def split_70_15_15(ids, seed=42):\n",
    "    if len(ids) == 0:\n",
    "        return [], [], []\n",
    "    tr_ids, temp_ids = train_test_split(ids, test_size=0.30, random_state=seed, shuffle=True)\n",
    "    va_ids, te_ids = train_test_split(temp_ids, test_size=0.50, random_state=seed, shuffle=True)\n",
    "    return list(tr_ids), list(va_ids), list(te_ids)\n",
    "\n",
    "tr_1, va_1, te_1     = split_70_15_15(ids_1mg,  seed=rng)\n",
    "tr_3, va_3, te_3     = split_70_15_15(ids_3mg,  seed=rng)\n",
    "tr_10, va_10, te_10  = split_70_15_15(ids_10mg, seed=rng)\n",
    "\n",
    "ids_tr = set(tr_1 + tr_3 + tr_10)\n",
    "ids_va = set(va_1 + va_3 + va_10)\n",
    "ids_te = set(te_1 + te_3 + te_10)\n",
    "\n",
    "sid_list = [s[0] for s in dataset_all.samples]\n",
    "tr_idx = [i for i, sid in enumerate(sid_list) if sid in ids_tr]\n",
    "va_idx = [i for i, sid in enumerate(sid_list) if sid in ids_va]\n",
    "te_idx = [i for i, sid in enumerate(sid_list) if sid in ids_te]\n",
    "\n",
    "train_ds = Subset(dataset_all, tr_idx)\n",
    "valid_ds = Subset(dataset_all, va_idx)\n",
    "test_ds  = Subset(dataset_all, te_idx)\n",
    "\n",
    "print(\"[ID split by fixed dose groups] (70/15/15)\")\n",
    "print(\" train IDs:\", len(ids_tr), \"| valid IDs:\", len(ids_va), \"| test IDs:\", len(ids_te))\n",
    "print(\" 1mg -> train/valid/test:\", len(tr_1), len(va_1), len(te_1))\n",
    "print(\" 3mg -> train/valid/test:\", len(tr_3), len(va_3), len(te_3))\n",
    "print(\"10mg -> train/valid/test:\", len(tr_10), len(va_10), len(te_10))\n",
    "print(f\"#samples -> train: {len(train_ds)} | valid: {len(valid_ds)} | test: {len(test_ds)}\")\n",
    "\n",
    "# -------- MLP 모델 --------\n",
    "class PDMLPClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    입력 x=[E(t), (BW-mean)/10, COMED] -> [Linear+ReLU(+Dropout)]*n_layers -> Linear(→1)\n",
    "    노출 링크: log(tau) = b0 + b1*((BW-mean)/10) + b2*COMED\n",
    "    \"\"\"\n",
    "    def __init__(self, bw_mean: float=70.0, hidden: int=32, n_layers: int=3, dropout_p: float=0.2):\n",
    "        super().__init__()\n",
    "        self.b0 = nn.Parameter(torch.tensor(math.log(24.0)))\n",
    "        self.b1 = nn.Parameter(torch.tensor(0.0))\n",
    "        self.b2 = nn.Parameter(torch.tensor(0.0))\n",
    "        self.bw_mean = float(bw_mean)\n",
    "\n",
    "        layers = []\n",
    "        in_dim = 3\n",
    "        for i in range(n_layers):\n",
    "            layers += [\n",
    "                nn.Linear(in_dim if i==0 else hidden, hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=dropout_p) if dropout_p and dropout_p > 0 else nn.Identity(),\n",
    "            ]\n",
    "        layers += [nn.Linear(hidden if n_layers>0 else in_dim, 1)]\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def _tau(self, bw: torch.Tensor, comed: torch.Tensor):\n",
    "        bwc = (bw - self.bw_mean) / 10.0\n",
    "        log_tau = self.b0 + self.b1*bwc + self.b2*comed\n",
    "        return torch.exp(log_tau).clamp_min(1.0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def tau_from_cov_np(self, bw_np: np.ndarray, cm_np: np.ndarray, device=None):\n",
    "        device = device or next(self.parameters()).device\n",
    "        bw = torch.tensor(bw_np, dtype=torch.float32, device=device)\n",
    "        cm = torch.tensor(cm_np, dtype=torch.float32, device=device)\n",
    "        return self._tau(bw, cm).detach().cpu().numpy()\n",
    "\n",
    "    def forward_single(self, t: torch.Tensor, dose_t: torch.Tensor, dose_a: torch.Tensor,\n",
    "                       bw: float, comed: float):\n",
    "        tau = self._tau(torch.tensor(bw, dtype=torch.float32, device=t.device),\n",
    "                        torch.tensor(comed, dtype=torch.float32, device=t.device))\n",
    "        dt = t - dose_t\n",
    "        mask = (dt >= 0).float()\n",
    "        exposure = (dose_a * torch.exp(-dt.clamp_min(0) / tau) * mask).sum()\n",
    "        x = torch.stack([\n",
    "            exposure,\n",
    "            (torch.tensor(bw, dtype=torch.float32, device=t.device) - self.bw_mean) / 10.0,\n",
    "            torch.tensor(comed, dtype=torch.float32, device=t.device)\n",
    "        ])\n",
    "        return self.mlp(x).squeeze()\n",
    "\n",
    "def _collate(batch): return batch\n",
    "\n",
    "# -------- 지표 계산/도움 함수 --------\n",
    "def _compute_metrics(y_true: np.ndarray, prob: np.ndarray, thr: float = 0.5):\n",
    "    pred = (prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, pred) if len(y_true) else float(\"nan\")\n",
    "    prec = precision_score(y_true, pred, zero_division=0)\n",
    "    rec  = recall_score(y_true, pred, zero_division=0)\n",
    "    f1   = f1_score(y_true, pred, zero_division=0)\n",
    "    try:\n",
    "        roc = roc_auc_score(y_true, prob) if (len(np.unique(y_true))>1) else float(\"nan\")\n",
    "    except Exception:\n",
    "        roc = float(\"nan\")\n",
    "    try:\n",
    "        ap  = average_precision_score(y_true, prob) if (len(np.unique(y_true))>1) else float(\"nan\")\n",
    "    except Exception:\n",
    "        ap  = float(\"nan\")\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, pred, labels=[0,1]).ravel()\n",
    "    except Exception:\n",
    "        tn=fp=fn=tp = 0\n",
    "    return {\"acc\":acc, \"prec\":prec, \"rec\":rec, \"f1\":f1, \"roc_auc\":roc, \"pr_auc\":ap,\n",
    "            \"tn\":tn, \"fp\":fp, \"fn\":fn, \"tp\":tp}\n",
    "\n",
    "def _predict_dataset(model, dataset, batch_size=512):\n",
    "    device = next(model.parameters()).device\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=_collate)\n",
    "    ys, ps = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            for b in batch:\n",
    "                logit = model.forward_single(\n",
    "                    b[\"t\"].to(device), b[\"dose_t\"].to(device), b[\"dose_a\"].to(device),\n",
    "                    float(b[\"bw\"]), float(b[\"cm\"])\n",
    "                )\n",
    "                ys.append(float(b[\"y\"]))\n",
    "                ps.append(torch.sigmoid(logit).item())\n",
    "    return np.array(ys), np.array(ps)\n",
    "\n",
    "def find_best_threshold(y, p, grid=None, metric=\"f1\"):\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.05, 0.95, 91)  # 0.05~0.95\n",
    "    best_thr, best_val = 0.5, -1.0\n",
    "    for thr in grid:\n",
    "        m = _compute_metrics(y, p, thr=thr)\n",
    "        val = m[\"f1\"] if metric==\"f1\" else (0.5*m[\"prec\"] + 0.5*m[\"rec\"])\n",
    "        if val > best_val:\n",
    "            best_val, best_thr = val, thr\n",
    "    return best_thr, best_val\n",
    "\n",
    "# -------- 학습 루틴(베스트 스냅샷/ES/스케줄러/WD) --------\n",
    "def train_classifier(train_ds, valid_ds,\n",
    "                     epochs=60, lr=5e-2, seed=42,\n",
    "                     mlp_hidden=32, mlp_layers=3, mlp_dropout=0.2,\n",
    "                     weight_decay=0.0, patience=10,\n",
    "                     sched_factor=0.5, sched_patience=5,\n",
    "                     clip_norm=None):\n",
    "    device = DEVICE\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    tr_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=_collate, generator=g)\n",
    "    va_loader = DataLoader(valid_ds, batch_size=256, shuffle=False, collate_fn=_collate)\n",
    "\n",
    "    bw_mean = float(np.mean([b[\"bw\"].item() for b in train_ds]))\n",
    "    model = PDMLPClassifier(bw_mean=bw_mean, hidden=mlp_hidden, n_layers=mlp_layers, dropout_p=mlp_dropout).to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    # ReduceLROnPlateau: 검증 F1 기준, 상승 없으면 LR 감소\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt, mode='max', factor=sched_factor, patience=sched_patience, verbose=True\n",
    "    )\n",
    "\n",
    "    best = (-1.0, None)\n",
    "    best_state = None\n",
    "    es_counter = 0  # EarlyStopping 카운터\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for batch in tr_loader:\n",
    "            opt.zero_grad()\n",
    "            loss = 0.0\n",
    "            for b in batch:\n",
    "                logit = model.forward_single(\n",
    "                    b[\"t\"].to(device), b[\"dose_t\"].to(device), b[\"dose_a\"].to(device),\n",
    "                    float(b[\"bw\"]), float(b[\"cm\"])\n",
    "                )\n",
    "                loss = loss + loss_fn(logit.view(()), b[\"y\"].to(device).view(()))\n",
    "            loss = loss/len(batch)\n",
    "            loss.backward()\n",
    "            if clip_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "            opt.step()\n",
    "\n",
    "        # ---- validation ----\n",
    "        y, p = _predict_dataset(model, valid_ds, batch_size=256)\n",
    "        # 모니터링 지표: F1(thr=0.5). (임계값 튜닝은 학습 후 별도)\n",
    "        metrics = _compute_metrics(y, p, thr=0.5)\n",
    "\n",
    "        # 스케줄러 스텝(F1 기준)\n",
    "        scheduler.step(metrics[\"f1\"])\n",
    "\n",
    "        # 베스트 갱신/스냅샷\n",
    "        if metrics[\"f1\"] > best[0]:\n",
    "            best = (metrics[\"f1\"], {\"epoch\":ep, **metrics})\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            es_counter = 0\n",
    "        else:\n",
    "            es_counter += 1\n",
    "\n",
    "        if ep%10==0 or ep<=3:\n",
    "            lr_cur = opt.param_groups[0][\"lr\"]\n",
    "            print(f\"[ep {ep:03d}] lr={lr_cur:.5f} acc={metrics['acc']:.3f} f1={metrics['f1']:.3f} \"\n",
    "                  f\"prec={metrics['prec']:.3f} rec={metrics['rec']:.3f} \"\n",
    "                  f\"roc_auc={metrics['roc_auc']:.3f} pr_auc={metrics['pr_auc']:.3f}\")\n",
    "\n",
    "        # EarlyStopping\n",
    "        if es_counter >= patience:\n",
    "            print(f\"EarlyStopping at epoch {ep} (no F1 improvement for {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "    # 베스트로 복원\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    print(\"best(valid @thr=0.5):\", best[1])\n",
    "\n",
    "    # ---- 검증셋에서 임계값 최적화 (베스트 모델로) ----\n",
    "    y_val, p_val = _predict_dataset(model, valid_ds, batch_size=256)\n",
    "    best_thr, best_f1 = find_best_threshold(y_val, p_val, grid=None, metric=\"f1\")\n",
    "    tuned_metrics = _compute_metrics(y_val, p_val, thr=best_thr)\n",
    "    print(f\"tuned threshold on valid: thr={best_thr:.3f} | F1={tuned_metrics['f1']:.4f} \"\n",
    "          f\"(prec={tuned_metrics['prec']:.4f}, rec={tuned_metrics['rec']:.4f})\")\n",
    "\n",
    "    return model, best[1], best_thr\n",
    "\n",
    "# -------- 인구 공변량 샘플러 --------\n",
    "def subject_cov_sampler(pd_df: pd.DataFrame, col_BW: Optional[str], col_COMED: Optional[str],\n",
    "                        n:int, scenario:str=\"base\", rng_seed:int=123):\n",
    "    obs_bw = pd_df[col_BW].dropna().to_numpy(dtype=float) if col_BW is not None else np.full(len(pd_df), 80.0)\n",
    "    obs_cm = pd_df[col_COMED].dropna().to_numpy(dtype=float) if col_COMED is not None else np.zeros(len(pd_df))\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    if scenario==\"base\":\n",
    "        idx = rng.integers(0, len(obs_bw), size=n)\n",
    "        return obs_bw[idx], obs_cm[idx]\n",
    "    elif scenario==\"bw_wide\":\n",
    "        bw = rng.uniform(70.0, 140.0, size=n)   # 필요시 관찰 범위로 조정 가능\n",
    "        cm = obs_cm[rng.integers(0, len(obs_cm), size=n)] if len(obs_cm)>0 else np.zeros(n)\n",
    "        return bw, cm\n",
    "    elif scenario==\"no_comed\":\n",
    "        idx = rng.integers(0, len(obs_bw), size=n)\n",
    "        return obs_bw[idx], np.zeros(n)\n",
    "    else:\n",
    "        raise ValueError(\"unknown scenario\")\n",
    "\n",
    "# -------- 정상상태(SS) 평가: MLP --------\n",
    "@torch.no_grad()\n",
    "def success_fraction_for_dose_ss_mlp(model: PDMLPClassifier, dose_mg: float, freq_h: int,\n",
    "                                     last_window_h: int, pd_df, col_BW: Optional[str], col_COMED: Optional[str],\n",
    "                                     Nsubj=300, scenario=\"base\", decision_threshold=0.5,\n",
    "                                     grid_step_h=1.0) -> float:\n",
    "    bw_arr, cm_arr = subject_cov_sampler(pd_df, col_BW, col_COMED, Nsubj, scenario=scenario)\n",
    "    tau = model.tau_from_cov_np(bw_arr, cm_arr, device=next(model.parameters()).device)\n",
    "    tgrid = np.arange(0.0, last_window_h + 1e-6, grid_step_h, dtype=float)\n",
    "    ok = 0\n",
    "    for i in range(Nsubj):\n",
    "        denom = (1.0 - np.exp(-float(freq_h) / max(tau[i],1e-6)))\n",
    "        denom = max(denom, 1e-6)\n",
    "        e_t = dose_mg * np.exp(-tgrid / max(tau[i],1e-6)) / denom\n",
    "        bwc = (bw_arr[i] - model.bw_mean) / 10.0\n",
    "        cm  = cm_arr[i]\n",
    "        X = torch.tensor(np.stack([e_t, np.full_like(e_t, bwc), np.full_like(e_t, cm)], axis=1),\n",
    "                         dtype=torch.float32, device=next(model.parameters()).device)\n",
    "        logits = model.mlp(X).squeeze(1)\n",
    "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        # 윈도우 전체 시간에서 성공 확률이 임계 이상이면 성공(아주 보수적 기준)\n",
    "        if probs.min() >= decision_threshold:\n",
    "            ok += 1\n",
    "    return ok / Nsubj\n",
    "\n",
    "def search_min_dose_ss(model: PDMLPClassifier, grid, freq_h, last_window_h, pd_df,\n",
    "                       col_BW: Optional[str], col_COMED: Optional[str],\n",
    "                       Nsubj=300, scenario=\"base\", target=0.9, decision_threshold=0.5):\n",
    "    rows = []\n",
    "    for d in grid:\n",
    "        frac = success_fraction_for_dose_ss_mlp(model, d, freq_h, last_window_h, pd_df, col_BW, col_COMED,\n",
    "                                                Nsubj=Nsubj, scenario=scenario, decision_threshold=decision_threshold)\n",
    "        rows.append({\"dose\": d, \"fraction\": frac})\n",
    "    df_res = pd.DataFrame(rows).sort_values(\"dose\")\n",
    "    feas = df_res[df_res[\"fraction\"]>=target]\n",
    "    best = feas.iloc[0][\"dose\"] if len(feas)>0 else None\n",
    "    return df_res, best\n",
    "\n",
    "# -------- 학습 실행 (개선 루틴) --------\n",
    "model, valid_best, tuned_thr = train_classifier(\n",
    "    train_ds, valid_ds,\n",
    "    epochs=EPOCHS, lr=LR,\n",
    "    mlp_hidden=MLP_HIDDEN, mlp_layers=MLP_LAYERS, mlp_dropout=MLP_DROPOUT,\n",
    "    weight_decay=WEIGHT_DECAY, patience=PATIENCE,\n",
    "    sched_factor=SCHED_FACTOR, sched_patience=SCHED_PATIENCE,\n",
    "    clip_norm=CLIP_NORM\n",
    ")\n",
    "print(\"\\nValidation best @thr=0.5:\", valid_best)\n",
    "print(f\"Validated tuned decision threshold: {tuned_thr:.3f}\")\n",
    "\n",
    "# -------- 용량 탐색 (튜닝 임계값 적용) --------\n",
    "daily_grid  = [0.5*i for i in range(0, 121)]   # 0..60 mg, 0.5 mg\n",
    "weekly_grid = [5*i   for i in range(0, 41)]    # 0..200 mg, 5 mg\n",
    "\n",
    "daily_base,  best_daily_base  = search_min_dose_ss(model, daily_grid,  24, 24,  pdf, col_BW, col_COMED,\n",
    "                                                   Nsubj=N_SUBJ, scenario=\"base\",\n",
    "                                                   target=0.90, decision_threshold=tuned_thr)\n",
    "weekly_base, best_weekly_base = search_min_dose_ss(model, weekly_grid, 168, 168, pdf, col_BW, col_COMED,\n",
    "                                                   Nsubj=N_SUBJ, scenario=\"base\",\n",
    "                                                   target=0.90, decision_threshold=tuned_thr)\n",
    "\n",
    "daily_bw,   best_daily_bw   = search_min_dose_ss(model, daily_grid,  24, 24,  pdf, col_BW, col_COMED,\n",
    "                                                 Nsubj=N_SUBJ, scenario=\"bw_wide\",\n",
    "                                                 target=0.90, decision_threshold=tuned_thr)\n",
    "weekly_bw,  best_weekly_bw  = search_min_dose_ss(model, weekly_grid, 168, 168, pdf, col_BW, col_COMED,\n",
    "                                                 Nsubj=N_SUBJ, scenario=\"bw_wide\",\n",
    "                                                 target=0.90, decision_threshold=tuned_thr)\n",
    "\n",
    "daily_nocm, best_daily_nocm = search_min_dose_ss(model, daily_grid,  24, 24,  pdf, col_BW, col_COMED,\n",
    "                                                 Nsubj=N_SUBJ, scenario=\"no_comed\",\n",
    "                                                 target=0.90, decision_threshold=tuned_thr)\n",
    "weekly_nocm,best_weekly_nocm= search_min_dose_ss(model, weekly_grid, 168, 168, pdf, col_BW, col_COMED,\n",
    "                                                 Nsubj=N_SUBJ, scenario=\"no_comed\",\n",
    "                                                 target=0.90, decision_threshold=tuned_thr)\n",
    "\n",
    "daily_75,   best_daily_75   = search_min_dose_ss(model, daily_grid,  24, 24,  pdf, col_BW, col_COMED,\n",
    "                                                 Nsubj=N_SUBJ, scenario=\"base\",\n",
    "                                                 target=0.75, decision_threshold=tuned_thr)\n",
    "weekly_75,  best_weekly_75  = search_min_dose_ss(model, weekly_grid, 168, 168, pdf, col_BW, col_COMED,\n",
    "                                                 Nsubj=N_SUBJ, scenario=\"base\",\n",
    "                                                 target=0.75, decision_threshold=tuned_thr)\n",
    "\n",
    "summary = pd.DataFrame([\n",
    "    {\"scenario\":\"Base (Phase 1-like)\", \"target\":\"90%\", \"once-daily (mg)\": best_daily_base,  \"once-weekly (mg)\": best_weekly_base},\n",
    "    {\"scenario\":\"BW 70–140 kg\",        \"target\":\"90%\", \"once-daily (mg)\": best_daily_bw,    \"once-weekly (mg)\": best_weekly_bw},\n",
    "    {\"scenario\":\"No COMED allowed\",    \"target\":\"90%\", \"once-daily (mg)\": best_daily_nocm,  \"once-weekly (mg)\": best_weekly_nocm},\n",
    "    {\"scenario\":\"Base (Phase 1-like)\", \"target\":\"75%\", \"once-daily (mg)\": best_daily_75,    \"once-weekly (mg)\": best_weekly_75},\n",
    "])\n",
    "print(\"\\n=== Dose recommendations summary (thr tuned) ===\")\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "# -------- 테스트 평가 (튜닝 임계값 적용) --------\n",
    "def evaluate_dataset(model, dataset, threshold=0.5, batch_size=512):\n",
    "    y, p = _predict_dataset(model, dataset, batch_size=batch_size)\n",
    "    return _compute_metrics(y, p, thr=threshold)\n",
    "\n",
    "test_metrics = evaluate_dataset(model, test_ds, threshold=tuned_thr)\n",
    "print(\"\\n=== Final TEST metrics (unseen IDs, tuned thr) ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"{k:>8s}: {v:.4f}\")\n",
    "    else:\n",
    "        print(f\"{k:>8s}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d0e96-1762-441c-bc46-94634a3282cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5189cc94-a1b8-4468-990a-29dba553b399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a91bf64c-13c4-49cd-bc51-fad824b35c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected columns: {'ID': 'ID', 'TIME': 'TIME', 'DV': 'DV', 'EVID': 'EVID', 'AMT': 'AMT', 'BW': 'BW', 'COMED': 'COMED'}\n",
      "PD rows: 1200 Dose rows: 756\n",
      "\n",
      "=== [TEST SEED=10] New random split (70/15/15 per dose group) ===\n",
      " train IDs: 24 | valid IDs: 6 | test IDs: 6\n",
      "#samples -> train: 600 | valid: 150 | test: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST SEED=10] CV tuned threshold (median over inner seed×fold): 0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inner CV summary] (mean over inner seed×fold)\n",
      "F1(mean)         0.819886\n",
      "PR-AUC(mean)     0.841951\n",
      "ROC-AUC(mean)    0.921435\n",
      "dtype: float64\n",
      "\n",
      "=== [TEST SEED=11] New random split (70/15/15 per dose group) ===\n",
      " train IDs: 24 | valid IDs: 6 | test IDs: 6\n",
      "#samples -> train: 600 | valid: 150 | test: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST SEED=11] CV tuned threshold (median over inner seed×fold): 0.490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inner CV summary] (mean over inner seed×fold)\n",
      "F1(mean)         0.860310\n",
      "PR-AUC(mean)     0.886408\n",
      "ROC-AUC(mean)    0.957458\n",
      "dtype: float64\n",
      "\n",
      "=== [TEST SEED=12] New random split (70/15/15 per dose group) ===\n",
      " train IDs: 24 | valid IDs: 6 | test IDs: 6\n",
      "#samples -> train: 600 | valid: 150 | test: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST SEED=12] CV tuned threshold (median over inner seed×fold): 0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inner CV summary] (mean over inner seed×fold)\n",
      "F1(mean)         0.779051\n",
      "PR-AUC(mean)     0.809282\n",
      "ROC-AUC(mean)    0.894194\n",
      "dtype: float64\n",
      "\n",
      "=== [TEST SEED=13] New random split (70/15/15 per dose group) ===\n",
      " train IDs: 24 | valid IDs: 6 | test IDs: 6\n",
      "#samples -> train: 600 | valid: 150 | test: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST SEED=13] CV tuned threshold (median over inner seed×fold): 0.490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inner CV summary] (mean over inner seed×fold)\n",
      "F1(mean)         0.836191\n",
      "PR-AUC(mean)     0.859612\n",
      "ROC-AUC(mean)    0.926580\n",
      "dtype: float64\n",
      "\n",
      "=== [TEST SEED=14] New random split (70/15/15 per dose group) ===\n",
      " train IDs: 24 | valid IDs: 6 | test IDs: 6\n",
      "#samples -> train: 600 | valid: 150 | test: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST SEED=14] CV tuned threshold (median over inner seed×fold): 0.480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Inner CV summary] (mean over inner seed×fold)\n",
      "F1(mean)         0.795207\n",
      "PR-AUC(mean)     0.814495\n",
      "ROC-AUC(mean)    0.903829\n",
      "dtype: float64\n",
      "\n",
      "=== Repeated RANDOM TEST splits summary ===\n",
      " test_seed  thr_cv_final  test_seed  test_acc  test_prec  test_rec  test_f1  test_roc_auc  test_pr_auc  test_tn  test_fp  test_fn  test_tp\n",
      "        10          0.50         10  0.866667   0.558140  0.960000 0.705882      0.967680     0.903632      106       19        1       24\n",
      "        11          0.49         11  0.946667   0.818182  1.000000 0.900000      0.998538     0.996032      106        8        0       36\n",
      "        12          0.50         12  0.980000   1.000000  0.911765 0.953846      0.998225     0.994332      116        0        3       31\n",
      "        13          0.49         13  0.906667   0.733333  0.785714 0.758621      0.958138     0.850582      114        8        6       22\n",
      "        14          0.48         14  0.953333   0.931034  0.843750 0.885246      0.991261     0.971836      116        2        5       27\n",
      "\n",
      "[Aggregate over test splits] mean ± std\n",
      "                    mean       std\n",
      "test_seed      12.000000  1.581139\n",
      "test_acc        0.930667  0.044372\n",
      "test_prec       0.808138  0.173285\n",
      "test_rec        0.900246  0.086550\n",
      "test_f1         0.840719  0.103942\n",
      "test_roc_auc    0.982768  0.018668\n",
      "test_pr_auc     0.943283  0.063971\n",
      "test_tn       111.600000  5.176872\n",
      "test_fp         7.400000  7.402702\n",
      "test_fn         3.000000  2.549510\n",
      "test_tp        28.000000  5.612486\n",
      "\n",
      "Saved: results_random_test_repeats.csv\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# PD-only 분류 + 정상상태 용량탐색 (MLP) \n",
    "# └ 외부 루프: \"테스트셋\"을 여러 번 임의로 바꿔가며 반복 평가\n",
    "# └ 내부 루프: train+valid에서 K-Fold(Group=ID) + 시드 반복\n",
    "# ===========================\n",
    "import os, math, warnings, numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix\n",
    ")\n",
    "from collections import defaultdict\n",
    "from typing import Optional, List, Tuple\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "# -------- 경로/기본 설정 --------\n",
    "CSV = \"EstData.csv\"\n",
    "assert os.path.exists(CSV), f\"CSV not found at {CSV}\"\n",
    "\n",
    "PD_THRESHOLD = 3.3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 학습/최적화\n",
    "EPOCHS = 120\n",
    "LR = 5e-2\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 12\n",
    "SCHED_FACTOR = 0.5\n",
    "SCHED_PATIENCE = 5\n",
    "CLIP_NORM = 1.0\n",
    "\n",
    "# MLP 하이퍼파라미터\n",
    "MLP_HIDDEN = 36\n",
    "MLP_LAYERS = 2\n",
    "MLP_DROPOUT = 0.1\n",
    "\n",
    "# 내부 CV\n",
    "N_SPLITS = 5\n",
    "SEEDS_INNER = [42, 3407, 777, 2021, 123]   # 내부(K-Fold×Seed) 반복\n",
    "\n",
    "# 외부 테스트 분할 반복 (테스트셋을 바꿔가며)\n",
    "TEST_SEEDS = [10, 11, 12, 13, 14]          # 원하는 만큼 추가\n",
    "SAVE_CSV = True\n",
    "OUT_CSV = \"results_random_test_repeats.csv\"\n",
    "\n",
    "def set_global_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def _find_col_like(df, name_opts):\n",
    "    low = {c.lower(): c for c in df.columns}\n",
    "    for n in name_opts:\n",
    "        if n in low: return low[n]\n",
    "    return None\n",
    "\n",
    "# -------- 데이터 로딩/전처리 --------\n",
    "df = pd.read_csv(CSV)\n",
    "\n",
    "col_ID   = _find_col_like(df, [\"id\"])\n",
    "col_TIME = _find_col_like(df, [\"time\"])\n",
    "col_DVID = _find_col_like(df, [\"dvid\"])\n",
    "col_DV   = _find_col_like(df, [\"dv\",\"pd\",\"value\"])\n",
    "col_EVID = _find_col_like(df, [\"evid\"])\n",
    "col_AMT  = _find_col_like(df, [\"amt\",\"dose\",\"dosen\",\"doses\"])\n",
    "col_BW   = _find_col_like(df, [\"bw\",\"weight\",\"bodyweight\"])\n",
    "col_COMED= _find_col_like(df, [\"comed\",\"conmed\",\"concom\"])\n",
    "\n",
    "need = [col_ID, col_TIME, col_DV, col_AMT]\n",
    "miss = [n for n,v in zip([\"ID\",\"TIME\",\"DV\",\"AMT/DOSE\"], need) if v is None]\n",
    "if miss:\n",
    "    warnings.warn(f\"Columns missing (minimum required): {miss}\")\n",
    "\n",
    "# PD 프레임\n",
    "if col_DVID is not None and col_DV is not None:\n",
    "    pdf = df[df[col_DVID]==2].copy()\n",
    "else:\n",
    "    pdf = df.copy()\n",
    "\n",
    "# 투약 이벤트\n",
    "if col_EVID is not None:\n",
    "    dose_df = df[df[col_EVID]==1].copy()\n",
    "else:\n",
    "    dose_df = df[df[col_AMT].notna()].copy()\n",
    "\n",
    "# 숫자 변환\n",
    "for c in [col_TIME, col_DV, col_AMT, col_BW, col_COMED]:\n",
    "    if c is not None:\n",
    "        pdf[c] = pd.to_numeric(pdf[c], errors=\"coerce\")\n",
    "        dose_df[c] = pd.to_numeric(dose_df[c], errors=\"coerce\")\n",
    "\n",
    "# 정렬/필요 열\n",
    "keep_pd = [c for c in [col_ID,col_TIME,col_DV,col_BW,col_COMED] if c is not None]\n",
    "keep_dose = [c for c in [col_ID,col_TIME,col_AMT] if c is not None]\n",
    "pdf = pdf[keep_pd].dropna().sort_values([col_ID, col_TIME])\n",
    "dose_df = dose_df[keep_dose].dropna().sort_values([col_ID, col_TIME])\n",
    "\n",
    "print(\"Detected columns:\", dict(ID=col_ID, TIME=col_TIME, DV=col_DV, EVID=col_EVID, AMT=col_AMT, BW=col_BW, COMED=col_COMED))\n",
    "print(\"PD rows:\", len(pdf), \"Dose rows:\", len(dose_df))\n",
    "\n",
    "# -------- Dataset --------\n",
    "class PDSamples(Dataset):\n",
    "    def __init__(self, pd_df: pd.DataFrame, dose_df: pd.DataFrame,\n",
    "                 col_ID: str, col_TIME: str, col_DV: str,\n",
    "                 col_BW: Optional[str], col_COMED: Optional[str], pd_threshold: float=3.3):\n",
    "        self.col_ID, self.col_TIME, self.col_DV = col_ID, col_TIME, col_DV\n",
    "        self.col_BW, self.col_COMED = col_BW, col_COMED\n",
    "        self.pd_threshold = pd_threshold\n",
    "\n",
    "        d_groups = defaultdict(list)\n",
    "        for _, row in dose_df.iterrows():\n",
    "            d_groups[row[col_ID]].append((float(row[col_TIME]), float(row[col_AMT])))\n",
    "        self.dose_map = {k: (np.array([t for t,a in v], dtype=np.float32),\n",
    "                              np.array([a for t,a in v], dtype=np.float32)) for k,v in d_groups.items()}\n",
    "\n",
    "        feats = []\n",
    "        for _, row in pd_df.iterrows():\n",
    "            sid = row[col_ID]\n",
    "            if sid not in self.dose_map:\n",
    "                continue\n",
    "            t = float(row[col_TIME])\n",
    "            val = float(row[col_DV])\n",
    "            y = 1.0 if (val <= pd_threshold) else 0.0\n",
    "            bw = float(row[col_BW]) if col_BW is not None else 70.0\n",
    "            cm = float(row[col_COMED]) if col_COMED is not None else 0.0\n",
    "            feats.append((sid, t, y, bw, cm))\n",
    "        self.samples = feats\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        sid, t, y, bw, cm = self.samples[idx]\n",
    "        dose_t, dose_a = self.dose_map.get(sid, (np.zeros(0, dtype=np.float32), np.zeros(0, dtype=np.float32)))\n",
    "        return {\n",
    "            \"sid\": sid,\n",
    "            \"t\": torch.tensor(t, dtype=torch.float32),\n",
    "            \"y\": torch.tensor(y, dtype=torch.float32),\n",
    "            \"bw\": torch.tensor(bw, dtype=torch.float32),\n",
    "            \"cm\": torch.tensor(cm, dtype=torch.float32),\n",
    "            \"dose_t\": torch.tensor(dose_t, dtype=torch.float32),\n",
    "            \"dose_a\": torch.tensor(dose_a, dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "def _collate(batch): return batch\n",
    "dataset_all = PDSamples(pdf, dose_df, col_ID, col_TIME, col_DV, col_BW, col_COMED, pd_threshold=PD_THRESHOLD)\n",
    "\n",
    "# -------- 투약 군 ID (플라시보 제외) --------\n",
    "placebo_ids = set(range(1, 13))     # 제외\n",
    "ids_1mg     = list(range(13, 25))   # 후보\n",
    "ids_3mg     = list(range(25, 37))\n",
    "ids_10mg    = list(range(37, 49))\n",
    "\n",
    "ids_in_data = set(df[col_ID].unique())\n",
    "ids_1mg  = sorted(ids_in_data.intersection(ids_1mg))\n",
    "ids_3mg  = sorted(ids_in_data.intersection(ids_3mg))\n",
    "ids_10mg = sorted(ids_in_data.intersection(ids_10mg))\n",
    "\n",
    "def split_70_15_15(ids, seed=42):\n",
    "    if len(ids) == 0:\n",
    "        return [], [], []\n",
    "    tr_ids, temp_ids = train_test_split(ids, test_size=0.30, random_state=seed, shuffle=True)\n",
    "    va_ids, te_ids = train_test_split(temp_ids, test_size=0.50, random_state=seed, shuffle=True)\n",
    "    return list(tr_ids), list(va_ids), list(te_ids)\n",
    "\n",
    "def indices_by_ids(id_set: set) -> List[int]:\n",
    "    sids = [s[0] for s in dataset_all.samples]\n",
    "    return [i for i, sid in enumerate(sids) if sid in id_set]\n",
    "\n",
    "# -------- 모델/유틸 --------\n",
    "class PDMLPClassifier(nn.Module):\n",
    "    def __init__(self, bw_mean: float=70.0, hidden: int=32, n_layers: int=3, dropout_p: float=0.2):\n",
    "        super().__init__()\n",
    "        self.b0 = nn.Parameter(torch.tensor(math.log(24.0)))\n",
    "        self.b1 = nn.Parameter(torch.tensor(0.0))\n",
    "        self.b2 = nn.Parameter(torch.tensor(0.0))\n",
    "        self.bw_mean = float(bw_mean)\n",
    "\n",
    "        layers = []\n",
    "        in_dim = 3\n",
    "        for i in range(n_layers):\n",
    "            layers += [\n",
    "                nn.Linear(in_dim if i==0 else hidden, hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=dropout_p) if dropout_p and dropout_p > 0 else nn.Identity(),\n",
    "            ]\n",
    "        layers += [nn.Linear(hidden if n_layers>0 else in_dim, 1)]\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def _tau(self, bw: torch.Tensor, comed: torch.Tensor):\n",
    "        bwc = (bw - self.bw_mean) / 10.0\n",
    "        log_tau = self.b0 + self.b1*bwc + self.b2*comed\n",
    "        return torch.exp(log_tau).clamp_min(1.0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def tau_from_cov_np(self, bw_np: np.ndarray, cm_np: np.ndarray, device=None):\n",
    "        device = device or next(self.parameters()).device\n",
    "        bw = torch.tensor(bw_np, dtype=torch.float32, device=device)\n",
    "        cm = torch.tensor(cm_np, dtype=torch.float32, device=device)\n",
    "        return self._tau(bw, cm).detach().cpu().numpy()\n",
    "\n",
    "    def forward_single(self, t: torch.Tensor, dose_t: torch.Tensor, dose_a: torch.Tensor,\n",
    "                       bw: float, comed: float):\n",
    "        tau = self._tau(torch.tensor(bw, dtype=torch.float32, device=t.device),\n",
    "                        torch.tensor(comed, dtype=torch.float32, device=t.device))\n",
    "        dt = t - dose_t\n",
    "        mask = (dt >= 0).float()\n",
    "        exposure = (dose_a * torch.exp(-dt.clamp_min(0) / tau) * mask).sum()\n",
    "        x = torch.stack([\n",
    "            exposure,\n",
    "            (torch.tensor(bw, dtype=torch.float32, device=t.device) - self.bw_mean) / 10.0,\n",
    "            torch.tensor(comed, dtype=torch.float32, device=t.device)\n",
    "        ])\n",
    "        return self.mlp(x).squeeze()\n",
    "\n",
    "def _compute_metrics(y_true: np.ndarray, prob: np.ndarray, thr: float = 0.5):\n",
    "    pred = (prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, pred) if len(y_true) else float(\"nan\")\n",
    "    prec = precision_score(y_true, pred, zero_division=0)\n",
    "    rec  = recall_score(y_true, pred, zero_division=0)\n",
    "    f1   = f1_score(y_true, pred, zero_division=0)\n",
    "    try:\n",
    "        roc = roc_auc_score(y_true, prob) if (len(np.unique(y_true))>1) else float(\"nan\")\n",
    "    except Exception:\n",
    "        roc = float(\"nan\")\n",
    "    try:\n",
    "        ap  = average_precision_score(y_true, prob) if (len(np.unique(y_true))>1) else float(\"nan\")\n",
    "    except Exception:\n",
    "        ap  = float(\"nan\")\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, pred, labels=[0,1]).ravel()\n",
    "    except Exception:\n",
    "        tn=fp=fn=tp = 0\n",
    "    return {\"acc\":acc, \"prec\":prec, \"rec\":rec, \"f1\":f1, \"roc_auc\":roc, \"pr_auc\":ap,\n",
    "            \"tn\":tn, \"fp\":fp, \"fn\":fn, \"tp\":tp}\n",
    "\n",
    "def _predict_dataset(model, dataset, batch_size=512):\n",
    "    device = next(model.parameters()).device\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=_collate)\n",
    "    ys, ps = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            for b in batch:\n",
    "                logit = model.forward_single(\n",
    "                    b[\"t\"].to(device), b[\"dose_t\"].to(device), b[\"dose_a\"].to(device),\n",
    "                    float(b[\"bw\"]), float(b[\"cm\"])\n",
    "                )\n",
    "                ys.append(float(b[\"y\"]))\n",
    "                ps.append(torch.sigmoid(logit).item())\n",
    "    return np.array(ys), np.array(ps)\n",
    "\n",
    "def find_best_threshold(y, p, grid=None, metric=\"f1\"):\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.05, 0.95, 91)\n",
    "    best_thr, best_val = 0.5, -1.0\n",
    "    for thr in grid:\n",
    "        m = _compute_metrics(y, p, thr=thr)\n",
    "        val = m[\"f1\"] if metric==\"f1\" else (0.5*m[\"prec\"] + 0.5*m[\"rec\"])\n",
    "        if val > best_val:\n",
    "            best_val, best_thr = val, thr\n",
    "    return best_thr, best_val\n",
    "\n",
    "def train_classifier(train_ds, valid_ds,\n",
    "                     epochs=60, lr=5e-2, seed=42,\n",
    "                     mlp_hidden=32, mlp_layers=3, mlp_dropout=0.2,\n",
    "                     weight_decay=0.0, patience=10,\n",
    "                     sched_factor=0.5, sched_patience=5,\n",
    "                     clip_norm=None):\n",
    "    device = DEVICE\n",
    "    set_global_seed(seed)\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    tr_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=_collate, generator=g)\n",
    "    va_loader = DataLoader(valid_ds, batch_size=256, shuffle=False, collate_fn=_collate)\n",
    "\n",
    "    bw_mean = float(np.mean([b[\"bw\"].item() for b in train_ds]))\n",
    "    model = PDMLPClassifier(bw_mean=bw_mean, hidden=mlp_hidden, n_layers=mlp_layers, dropout_p=mlp_dropout).to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt, mode='max', factor=sched_factor, patience=sched_patience, verbose=False\n",
    "    )\n",
    "\n",
    "    best = (-1.0, None)\n",
    "    best_state = None\n",
    "    es_counter = 0\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for batch in tr_loader:\n",
    "            opt.zero_grad()\n",
    "            loss = 0.0\n",
    "            for b in batch:\n",
    "                logit = model.forward_single(\n",
    "                    b[\"t\"].to(device), b[\"dose_t\"].to(device), b[\"dose_a\"].to(device),\n",
    "                    float(b[\"bw\"]), float(b[\"cm\"])\n",
    "                )\n",
    "                loss = loss + loss_fn(logit.view(()), b[\"y\"].to(device).view(()))\n",
    "            loss = loss/len(batch)\n",
    "            loss.backward()\n",
    "            if clip_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "            opt.step()\n",
    "\n",
    "        # validation (모니터링만; 임계값 튜닝은 아래서 별도)\n",
    "        y, p = _predict_dataset(model, valid_ds, batch_size=256)\n",
    "        metrics = _compute_metrics(y, p, thr=0.5)\n",
    "        scheduler.step(metrics[\"f1\"])\n",
    "\n",
    "        if metrics[\"f1\"] > best[0]:\n",
    "            best = (metrics[\"f1\"], {\"epoch\":ep, **metrics})\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            es_counter = 0\n",
    "        else:\n",
    "            es_counter += 1\n",
    "\n",
    "        if es_counter >= patience:\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # 임계값 튜닝 (검증 확률 기반)\n",
    "    y_val, p_val = _predict_dataset(model, valid_ds, batch_size=256)\n",
    "    best_thr, _ = find_best_threshold(y_val, p_val, grid=None, metric=\"f1\")\n",
    "    tuned_metrics = _compute_metrics(y_val, p_val, thr=best_thr)\n",
    "\n",
    "    return model, best[1], best_thr, tuned_metrics\n",
    "\n",
    "# -------- CV 폴드 도우미 (용량군 균형 유지) --------\n",
    "def make_balanced_group_folds(ids_group: List[int], n_splits:int, seed:int) -> List[List[int]]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    ids = np.array(ids_group, dtype=int)\n",
    "    rng.shuffle(ids)\n",
    "    folds = np.array_split(ids, n_splits)\n",
    "    return [list(f) for f in folds]\n",
    "\n",
    "def make_cv_folds(ids_1: List[int], ids_3: List[int], ids_10: List[int],\n",
    "                  candidate_ids: List[int], n_splits:int, seed:int) -> List[Tuple[List[int], List[int]]]:\n",
    "    folds_1  = make_balanced_group_folds([i for i in ids_1  if i in candidate_ids], n_splits, seed)\n",
    "    folds_3  = make_balanced_group_folds([i for i in ids_3  if i in candidate_ids], n_splits, seed+1)\n",
    "    folds_10 = make_balanced_group_folds([i for i in ids_10 if i in candidate_ids], n_splits, seed+2)\n",
    "    folds = []\n",
    "    for k in range(n_splits):\n",
    "        val_ids = set(folds_1[k]) | set(folds_3[k]) | set(folds_10[k])\n",
    "        all_ids = set(candidate_ids)\n",
    "        tr_ids  = sorted(list(all_ids - val_ids))\n",
    "        folds.append((tr_ids, sorted(list(val_ids))))\n",
    "    return folds\n",
    "\n",
    "def subset_by_ids(ids: List[int]) -> Subset:\n",
    "    sids = [s[0] for s in dataset_all.samples]\n",
    "    idx = [i for i, sid in enumerate(sids) if sid in set(ids)]\n",
    "    return Subset(dataset_all, idx)\n",
    "\n",
    "def evaluate_dataset(model, dataset, threshold=0.5, batch_size=512):\n",
    "    y, p = _predict_dataset(model, dataset, batch_size=batch_size)\n",
    "    return _compute_metrics(y, p, thr=threshold), y, p\n",
    "\n",
    "# -------- 정상상태 시뮬레이션(옵션) --------\n",
    "def subject_cov_sampler(pd_df: pd.DataFrame, col_BW: Optional[str], col_COMED: Optional[str],\n",
    "                        n:int, scenario:str=\"base\", rng_seed:int=123):\n",
    "    obs_bw = pd_df[col_BW].dropna().to_numpy(dtype=float) if col_BW is not None else np.full(len(pd_df), 80.0)\n",
    "    obs_cm = pd_df[col_COMED].dropna().to_numpy(dtype=float) if col_COMED is not None else np.zeros(len(pd_df))\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    if scenario==\"base\":\n",
    "        idx = rng.integers(0, len(obs_bw), size=n)\n",
    "        return obs_bw[idx], obs_cm[idx]\n",
    "    elif scenario==\"bw_wide\":\n",
    "        bw = rng.uniform(70.0, 140.0, size=n)\n",
    "        cm = obs_cm[rng.integers(0, len(obs_cm), size=n)] if len(obs_cm)>0 else np.zeros(n)\n",
    "        return bw, cm\n",
    "    elif scenario==\"no_comed\":\n",
    "        idx = rng.integers(0, len(obs_bw), size=n)\n",
    "        return obs_bw[idx], np.zeros(n)\n",
    "    else:\n",
    "        raise ValueError(\"unknown scenario\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def success_fraction_for_dose_ss_mlp(model: PDMLPClassifier, dose_mg: float, freq_h: int,\n",
    "                                     last_window_h: int, pd_df, col_BW: Optional[str], col_COMED: Optional[str],\n",
    "                                     Nsubj=300, scenario=\"base\", decision_threshold=0.5,\n",
    "                                     grid_step_h=1.0) -> float:\n",
    "    bw_arr, cm_arr = subject_cov_sampler(pd_df, col_BW, col_COMED, Nsubj, scenario=scenario)\n",
    "    tau = model.tau_from_cov_np(bw_arr, cm_arr, device=next(model.parameters()).device)\n",
    "    tgrid = np.arange(0.0, last_window_h + 1e-6, grid_step_h, dtype=float)\n",
    "    ok = 0\n",
    "    for i in range(Nsubj):\n",
    "        denom = (1.0 - np.exp(-float(freq_h) / max(tau[i],1e-6)))\n",
    "        denom = max(denom, 1e-6)\n",
    "        e_t = dose_mg * np.exp(-tgrid / max(tau[i],1e-6)) / denom\n",
    "        bwc = (bw_arr[i] - model.bw_mean) / 10.0\n",
    "        cm  = cm_arr[i]\n",
    "        X = torch.tensor(np.stack([e_t, np.full_like(e_t, bwc), np.full_like(e_t, cm)], axis=1),\n",
    "                         dtype=torch.float32, device=next(model.parameters()).device)\n",
    "        logits = model.mlp(X).squeeze(1)\n",
    "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        if probs.min() >= decision_threshold:\n",
    "            ok += 1\n",
    "    return ok / Nsubj\n",
    "\n",
    "# ==========================================================\n",
    "# 메인: 외부 테스트 분할 반복 (TEST_SEEDS) × 내부 CV(SEEDS_INNER)\n",
    "# ==========================================================\n",
    "all_rows = []\n",
    "\n",
    "for test_seed in TEST_SEEDS:\n",
    "    # 1) 용량군별 70/15/15로 \"새로운\" train/valid/test 분할 생성\n",
    "    tr_1, va_1, te_1     = split_70_15_15(ids_1mg,  seed=test_seed)\n",
    "    tr_3, va_3, te_3     = split_70_15_15(ids_3mg,  seed=test_seed)\n",
    "    tr_10, va_10, te_10  = split_70_15_15(ids_10mg, seed=test_seed)\n",
    "\n",
    "    ids_tr = set(tr_1 + tr_3 + tr_10)\n",
    "    ids_va = set(va_1 + va_3 + va_10)\n",
    "    ids_te = set(te_1 + te_3 + te_10)\n",
    "\n",
    "    tr_idx = indices_by_ids(ids_tr)\n",
    "    va_idx = indices_by_ids(ids_va)\n",
    "    te_idx = indices_by_ids(ids_te)\n",
    "\n",
    "    train_ds_full = Subset(dataset_all, tr_idx)\n",
    "    valid_ds_full = Subset(dataset_all, va_idx)\n",
    "    test_ds       = Subset(dataset_all, te_idx)\n",
    "\n",
    "    print(f\"\\n=== [TEST SEED={test_seed}] New random split (70/15/15 per dose group) ===\")\n",
    "    print(\" train IDs:\", len(ids_tr), \"| valid IDs:\", len(ids_va), \"| test IDs:\", len(ids_te))\n",
    "    print(f\"#samples -> train: {len(train_ds_full)} | valid: {len(valid_ds_full)} | test: {len(test_ds)}\")\n",
    "\n",
    "    # 2) 내부 CV: train+valid 묶음으로 K-Fold(Group) 구성 (용량군 균형)\n",
    "    ids_trva = sorted(list(ids_tr | ids_va))\n",
    "    cv_folds = make_cv_folds(ids_1mg, ids_3mg, ids_10mg, ids_trva, N_SPLITS, seed=42)\n",
    "\n",
    "    # 3) 내부: 시드 반복 × KFold → 임계값 중앙값 산출\n",
    "    tuned_thresholds = []\n",
    "    cv_rows = []\n",
    "    for seed in SEEDS_INNER:\n",
    "        for k, (tr_ids_k, va_ids_k) in enumerate(cv_folds, start=1):\n",
    "            tr_ds_k = subset_by_ids(tr_ids_k)\n",
    "            va_ds_k = subset_by_ids(va_ids_k)\n",
    "            model_k, best_raw, tuned_thr_k, tuned_metrics_k = train_classifier(\n",
    "                tr_ds_k, va_ds_k,\n",
    "                epochs=EPOCHS, lr=LR, seed=seed,\n",
    "                mlp_hidden=MLP_HIDDEN, mlp_layers=MLP_LAYERS, mlp_dropout=MLP_DROPOUT,\n",
    "                weight_decay=WEIGHT_DECAY, patience=PATIENCE,\n",
    "                sched_factor=SCHED_FACTOR, sched_patience=SCHED_PATIENCE,\n",
    "                clip_norm=CLIP_NORM\n",
    "            )\n",
    "            tuned_thresholds.append(tuned_thr_k)\n",
    "            cv_rows.append({\n",
    "                \"test_seed\": test_seed,\n",
    "                \"inner_seed\": seed,\n",
    "                \"fold\": k,\n",
    "                \"val_acc\": tuned_metrics_k[\"acc\"],\n",
    "                \"val_prec\": tuned_metrics_k[\"prec\"],\n",
    "                \"val_rec\": tuned_metrics_k[\"rec\"],\n",
    "                \"val_f1\": tuned_metrics_k[\"f1\"],\n",
    "                \"val_roc_auc\": tuned_metrics_k[\"roc_auc\"],\n",
    "                \"val_pr_auc\": tuned_metrics_k[\"pr_auc\"],\n",
    "                \"thr\": tuned_thr_k\n",
    "            })\n",
    "\n",
    "    thr_cv_final = float(np.median(tuned_thresholds)) if len(tuned_thresholds)>0 else 0.5\n",
    "    print(f\"[TEST SEED={test_seed}] CV tuned threshold (median over inner seed×fold): {thr_cv_final:.3f}\")\n",
    "\n",
    "    # 4) train+valid 전체로 재학습 → 해당 test로 평가 (thr = thr_cv_final)\n",
    "    trainval_ds = Subset(dataset_all, tr_idx + va_idx)\n",
    "    final_model, _, _, _ = train_classifier(\n",
    "        trainval_ds, valid_ds_full,  # valid_ds_full은 모니터링용\n",
    "        epochs=EPOCHS, lr=LR, seed=SEEDS_INNER[0],\n",
    "        mlp_hidden=MLP_HIDDEN, mlp_layers=MLP_LAYERS, mlp_dropout=MLP_DROPOUT,\n",
    "        weight_decay=WEIGHT_DECAY, patience=PATIENCE,\n",
    "        sched_factor=SCHED_FACTOR, sched_patience=SCHED_PATIENCE,\n",
    "        clip_norm=CLIP_NORM\n",
    ")\n",
    "    test_metrics, y_te, p_te = evaluate_dataset(final_model, test_ds, threshold=thr_cv_final)\n",
    "\n",
    "    row = {\"test_seed\": test_seed, \"thr_cv_final\": thr_cv_final}\n",
    "    row.update({f\"test_{k}\": v for k, v in test_metrics.items()})\n",
    "    all_rows.append(row)\n",
    "\n",
    "    # (선택) 내부 CV 통계 출력\n",
    "    cv_df = pd.DataFrame(cv_rows)\n",
    "    if not cv_df.empty:\n",
    "        print(\"\\n[Inner CV summary] (mean over inner seed×fold)\")\n",
    "        print(cv_df[[\"val_f1\",\"val_pr_auc\",\"val_roc_auc\"]].mean().rename({\n",
    "            \"val_f1\":\"F1(mean)\",\"val_pr_auc\":\"PR-AUC(mean)\",\"val_roc_auc\":\"ROC-AUC(mean)\"\n",
    "        }))\n",
    "\n",
    "# -------- 반복 결과 요약 --------\n",
    "res_df = pd.DataFrame(all_rows)\n",
    "print(\"\\n=== Repeated RANDOM TEST splits summary ===\")\n",
    "if not res_df.empty:\n",
    "    cols_print = [c for c in res_df.columns if c.startswith(\"test_\")]  # test_지표만\n",
    "    print(res_df[[\"test_seed\",\"thr_cv_final\"] + cols_print].to_string(index=False))\n",
    "    print(\"\\n[Aggregate over test splits] mean ± std\")\n",
    "    agg = res_df[cols_print].agg(['mean','std']).T\n",
    "    print(agg.to_string())\n",
    "\n",
    "if SAVE_CSV and not res_df.empty:\n",
    "    res_df.to_csv(OUT_CSV, index=False)\n",
    "    print(f\"\\nSaved: {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c9e55c9-ce4c-4ace-9dfb-1a12f72ea050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected columns: {'ID': 'ID', 'TIME': 'TIME', 'DV': 'DV', 'EVID': 'EVID', 'AMT': 'AMT', 'BW': 'BW', 'COMED': 'COMED'}\n",
      "PD rows: 1200 Dose rows: 756\n",
      "[ID split by fixed dose groups] (70/15/15)\n",
      " train IDs: 24 | valid IDs: 6 | test IDs: 6\n",
      " 1mg -> train/valid/test: 8 2 2\n",
      " 3mg -> train/valid/test: 8 2 2\n",
      "10mg -> train/valid/test: 8 2 2\n",
      "#samples -> train: 600 | valid: 150 | test: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gold/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ep 001] lr=0.05000 acc=0.760 f1=0.471 prec=0.444 rec=0.500 roc_auc=0.750 pr_auc=0.527\n",
      "[ep 002] lr=0.05000 acc=0.787 f1=0.000 prec=0.000 rec=0.000 roc_auc=0.731 pr_auc=0.472\n",
      "[ep 003] lr=0.05000 acc=0.787 f1=0.000 prec=0.000 rec=0.000 roc_auc=0.697 pr_auc=0.484\n",
      "[ep 010] lr=0.02500 acc=0.853 f1=0.607 prec=0.708 rec=0.531 roc_auc=0.718 pr_auc=0.620\n",
      "[ep 020] lr=0.02500 acc=0.880 f1=0.690 prec=0.769 rec=0.625 roc_auc=0.852 pr_auc=0.737\n",
      "[ep 030] lr=0.02500 acc=0.913 f1=0.764 prec=0.913 rec=0.656 roc_auc=0.907 pr_auc=0.833\n",
      "[ep 040] lr=0.01250 acc=0.920 f1=0.793 prec=0.885 rec=0.719 roc_auc=0.962 pr_auc=0.914\n",
      "[ep 050] lr=0.00625 acc=0.933 f1=0.828 prec=0.923 rec=0.750 roc_auc=0.979 pr_auc=0.941\n",
      "EarlyStopping at epoch 59 (no F1 improvement for 12 epochs).\n",
      "best(valid @thr=0.5): {'epoch': 47, 'acc': 0.9333333333333333, 'prec': 0.8928571428571429, 'rec': 0.78125, 'f1': 0.8333333333333334, 'roc_auc': np.float64(0.982521186440678), 'pr_auc': np.float64(0.947851131556032), 'tn': np.int64(115), 'fp': np.int64(3), 'fn': np.int64(7), 'tp': np.int64(25)}\n",
      "tuned threshold on valid: thr=0.230 | F1=0.8493 (prec=0.7561, rec=0.9688)\n",
      "\n",
      "Validation best @thr=0.5: {'epoch': 47, 'acc': 0.9333333333333333, 'prec': 0.8928571428571429, 'rec': 0.78125, 'f1': 0.8333333333333334, 'roc_auc': np.float64(0.982521186440678), 'pr_auc': np.float64(0.947851131556032), 'tn': np.int64(115), 'fp': np.int64(3), 'fn': np.int64(7), 'tp': np.int64(25)}\n",
      "Validated tuned decision threshold: 0.230\n",
      "\n",
      "=== Dose recommendations summary (thr tuned) ===\n",
      "           scenario target  once-daily (mg)  once-weekly (mg)\n",
      "Base (Phase 1-like)    90%              4.5              50.0\n",
      "       BW 70–140 kg    90%              5.5              60.0\n",
      "   No COMED allowed    90%              5.0              55.0\n",
      "Base (Phase 1-like)    75%              3.0              40.0\n",
      "\n",
      "=== Final TEST metrics (unseen IDs, tuned thr) ===\n",
      "     acc: 0.8467\n",
      "    prec: 0.6207\n",
      "     rec: 0.9730\n",
      "      f1: 0.7579\n",
      " roc_auc: 0.9648\n",
      "  pr_auc: 0.9052\n",
      "      tn: 91\n",
      "      fp: 22\n",
      "      fn: 1\n",
      "      tp: 36\n"
     ]
    }
   ],
   "source": [
    "# residual 추가버전\n",
    "import os, math, warnings, numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix\n",
    ")\n",
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "# -------- 설정 --------\n",
    "CSV = \"EstData.csv\"       # 필요 시 절대경로로 교체\n",
    "PD_THRESHOLD = 3.3\n",
    "\n",
    "# 학습/최적화 설정\n",
    "EPOCHS = 120\n",
    "LR = 5e-2\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 12               # EarlyStopping 인내 에폭\n",
    "SCHED_FACTOR = 0.5          # ReduceLROnPlateau 감쇠 비율\n",
    "SCHED_PATIENCE = 5          # 스케줄러 인내 에폭(지표 기준)\n",
    "CLIP_NORM = 1.0             # 그라디언트 클리핑 (안쓰려면 None)\n",
    "\n",
    "# 시뮬레이션/검색 설정\n",
    "N_SUBJ = 300\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ===== 아래 두 줄을 설정 블록(MLP 하이퍼파라미터 근처)에 추가 =====\n",
    "USE_RESIDUAL = True      # True: 잔차 버전 / False: 기존(순차형) 버전\n",
    "RES_BLOCKS   = 2         # 잔차 버전일 때의 블록 수(= “층” 느낌). 2면 네 원래 2층 감각\n",
    "INPUT_LN     = True      # 잔차 버전일 때 입력 LayerNorm(3) 사용 여부\n",
    "\n",
    "\n",
    "# MLP 하이퍼파라미터\n",
    "MLP_HIDDEN = 36\n",
    "MLP_LAYERS = 2\n",
    "MLP_DROPOUT = 0.2\n",
    "\n",
    "# 재현성(완전 결정적 보장은 환경 의존. CUDA에서 완전 결정적 필요시 CUBLAS_WORKSPACE_CONFIG 설정 필요)\n",
    "def set_global_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # 완전 결정적이 필요하면 환경변수 설정 필요(CuBLAS). 여기서는 사용하지 않음.\n",
    "\n",
    "set_global_seed(42)\n",
    "assert os.path.exists(CSV), f\"CSV not found at {CSV}\"\n",
    "\n",
    "# -------- 유틸: 열 이름 추론 --------\n",
    "def _find_col_like(df, name_opts):\n",
    "    low = {c.lower(): c for c in df.columns}\n",
    "    for n in name_opts:\n",
    "        if n in low: return low[n]\n",
    "    return None\n",
    "\n",
    "# -------- 데이터 로딩/전처리 --------\n",
    "df = pd.read_csv(CSV)\n",
    "\n",
    "col_ID   = _find_col_like(df, [\"id\"])\n",
    "col_TIME = _find_col_like(df, [\"time\"])\n",
    "col_DVID = _find_col_like(df, [\"dvid\"])\n",
    "col_DV   = _find_col_like(df, [\"dv\",\"pd\",\"value\"])\n",
    "col_EVID = _find_col_like(df, [\"evid\"])\n",
    "col_AMT  = _find_col_like(df, [\"amt\",\"dose\",\"dosen\",\"doses\"])\n",
    "col_BW   = _find_col_like(df, [\"bw\",\"weight\",\"bodyweight\"])\n",
    "col_COMED= _find_col_like(df, [\"comed\",\"conmed\",\"concom\"])\n",
    "\n",
    "need = [col_ID, col_TIME, col_DV, col_AMT]\n",
    "miss = [n for n,v in zip([\"ID\",\"TIME\",\"DV\",\"AMT/DOSE\"], need) if v is None]\n",
    "if miss:\n",
    "    warnings.warn(f\"Columns missing (minimum required): {miss}\")\n",
    "\n",
    "# PD 행 추출 (DVID==2가 있으면 그걸 사용)\n",
    "if col_DVID is not None and col_DV is not None:\n",
    "    pdf = df[df[col_DVID]==2].copy()\n",
    "else:\n",
    "    pdf = df.copy()\n",
    "\n",
    "# 투약 이벤트 (EVID==1 우선, 아니면 AMT/DOSE notna)\n",
    "if col_EVID is not None:\n",
    "    dose_df = df[df[col_EVID]==1].copy()\n",
    "else:\n",
    "    dose_df = df[df[col_AMT].notna()].copy()\n",
    "\n",
    "# 숫자 변환\n",
    "for c in [col_TIME, col_DV, col_AMT, col_BW, col_COMED]:\n",
    "    if c is not None:\n",
    "        pdf[c] = pd.to_numeric(pdf[c], errors=\"coerce\")\n",
    "        dose_df[c] = pd.to_numeric(dose_df[c], errors=\"coerce\")\n",
    "\n",
    "# 정렬/필요 열만\n",
    "keep_pd = [c for c in [col_ID,col_TIME,col_DV,col_BW,col_COMED] if c is not None]\n",
    "keep_dose = [c for c in [col_ID,col_TIME,col_AMT] if c is not None]\n",
    "pdf = pdf[keep_pd].dropna().sort_values([col_ID, col_TIME])\n",
    "dose_df = dose_df[keep_dose].dropna().sort_values([col_ID, col_TIME])\n",
    "\n",
    "print(\"Detected columns:\", dict(ID=col_ID, TIME=col_TIME, DV=col_DV, EVID=col_EVID, AMT=col_AMT, BW=col_BW, COMED=col_COMED))\n",
    "print(\"PD rows:\", len(pdf), \"Dose rows:\", len(dose_df))\n",
    "\n",
    "# -------- 데이터셋 --------\n",
    "class PDSamples(Dataset):\n",
    "    def __init__(self, pd_df: pd.DataFrame, dose_df: pd.DataFrame,\n",
    "                 col_ID: str, col_TIME: str, col_DV: str,\n",
    "                 col_BW: Optional[str], col_COMED: Optional[str], pd_threshold: float=3.3):\n",
    "        self.col_ID, self.col_TIME, self.col_DV = col_ID, col_TIME, col_DV\n",
    "        self.col_BW, self.col_COMED = col_BW, col_COMED\n",
    "        self.pd_threshold = pd_threshold\n",
    "\n",
    "        # ID별 투약 히스토리\n",
    "        d_groups = defaultdict(list)\n",
    "        for _, row in dose_df.iterrows():\n",
    "            d_groups[row[col_ID]].append((float(row[col_TIME]), float(row[col_AMT])))\n",
    "        self.dose_map = {k: (np.array([t for t,a in v], dtype=np.float32),\n",
    "                              np.array([a for t,a in v], dtype=np.float32)) for k,v in d_groups.items()}\n",
    "\n",
    "        # 샘플: 각 PD 관측 시점\n",
    "        feats = []\n",
    "        for _, row in pd_df.iterrows():\n",
    "            sid = row[col_ID]\n",
    "            if sid not in self.dose_map:\n",
    "                continue\n",
    "            t = float(row[col_TIME])\n",
    "            val = float(row[col_DV])\n",
    "            y = 1.0 if (val <= pd_threshold) else 0.0\n",
    "            bw = float(row[col_BW]) if col_BW is not None else 70.0\n",
    "            cm = float(row[col_COMED]) if col_COMED is not None else 0.0\n",
    "            feats.append((sid, t, y, bw, cm))\n",
    "        self.samples = feats\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sid, t, y, bw, cm = self.samples[idx]\n",
    "        dose_t, dose_a = self.dose_map.get(sid, (np.zeros(0, dtype=np.float32), np.zeros(0, dtype=np.float32)))\n",
    "        return {\n",
    "            \"t\": torch.tensor(t, dtype=torch.float32),\n",
    "            \"y\": torch.tensor(y, dtype=torch.float32),\n",
    "            \"bw\": torch.tensor(bw, dtype=torch.float32),\n",
    "            \"cm\": torch.tensor(cm, dtype=torch.float32),\n",
    "            \"dose_t\": torch.tensor(dose_t, dtype=torch.float32),\n",
    "            \"dose_a\": torch.tensor(dose_a, dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "def _collate(batch): return batch\n",
    "\n",
    "dataset_all = PDSamples(pdf, dose_df, col_ID, col_TIME, col_DV, col_BW, col_COMED, pd_threshold=PD_THRESHOLD)\n",
    "\n",
    "# -------- ID 단위 분할: 플라시보 제외 + 1/3/10 mg 그룹별 70/15/15 --------\n",
    "rng = 42\n",
    "placebo_ids = set(range(1, 13))     # 0 mg (제외)\n",
    "ids_1mg     = list(range(13, 25))   # 1 mg\n",
    "ids_3mg     = list(range(25, 37))   # 3 mg\n",
    "ids_10mg    = list(range(37, 49))   # 10 mg\n",
    "\n",
    "ids_in_data = set(df[col_ID].unique())\n",
    "ids_1mg  = sorted(ids_in_data.intersection(ids_1mg))\n",
    "ids_3mg  = sorted(ids_in_data.intersection(ids_3mg))\n",
    "ids_10mg = sorted(ids_in_data.intersection(ids_10mg))\n",
    "\n",
    "def split_70_15_15(ids, seed=42):\n",
    "    if len(ids) == 0:\n",
    "        return [], [], []\n",
    "    tr_ids, temp_ids = train_test_split(ids, test_size=0.30, random_state=seed, shuffle=True)\n",
    "    va_ids, te_ids = train_test_split(temp_ids, test_size=0.50, random_state=seed, shuffle=True)\n",
    "    return list(tr_ids), list(va_ids), list(te_ids)\n",
    "\n",
    "tr_1, va_1, te_1     = split_70_15_15(ids_1mg,  seed=rng)\n",
    "tr_3, va_3, te_3     = split_70_15_15(ids_3mg,  seed=rng)\n",
    "tr_10, va_10, te_10  = split_70_15_15(ids_10mg, seed=rng)\n",
    "\n",
    "ids_tr = set(tr_1 + tr_3 + tr_10)\n",
    "ids_va = set(va_1 + va_3 + va_10)\n",
    "ids_te = set(te_1 + te_3 + te_10)\n",
    "\n",
    "sid_list = [s[0] for s in dataset_all.samples]\n",
    "tr_idx = [i for i, sid in enumerate(sid_list) if sid in ids_tr]\n",
    "va_idx = [i for i, sid in enumerate(sid_list) if sid in ids_va]\n",
    "te_idx = [i for i, sid in enumerate(sid_list) if sid in ids_te]\n",
    "\n",
    "train_ds = Subset(dataset_all, tr_idx)\n",
    "valid_ds = Subset(dataset_all, va_idx)\n",
    "test_ds  = Subset(dataset_all, te_idx)\n",
    "\n",
    "print(\"[ID split by fixed dose groups] (70/15/15)\")\n",
    "print(\" train IDs:\", len(ids_tr), \"| valid IDs:\", len(ids_va), \"| test IDs:\", len(ids_te))\n",
    "print(\" 1mg -> train/valid/test:\", len(tr_1), len(va_1), len(te_1))\n",
    "print(\" 3mg -> train/valid/test:\", len(tr_3), len(va_3), len(te_3))\n",
    "print(\"10mg -> train/valid/test:\", len(tr_10), len(va_10), len(te_10))\n",
    "print(f\"#samples -> train: {len(train_ds)} | valid: {len(valid_ds)} | test: {len(test_ds)}\")\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, h, p=0.2):\n",
    "        super().__init__()\n",
    "        self.ln  = nn.LayerNorm(h)\n",
    "        self.fc  = nn.Linear(h, h)\n",
    "        self.act = nn.ReLU()\n",
    "        self.do  = nn.Dropout(p) if p and p>0 else nn.Identity()\n",
    "    def forward(self, x):\n",
    "        r = x\n",
    "        x = self.ln(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.do(x)\n",
    "        return x + r\n",
    "\n",
    "class PDMLPClassifierResidual(nn.Module):\n",
    "    \"\"\"\n",
    "    입력 x=[E(t), (BW-mean)/10, COMED]\n",
    "    구조: in_ln(선택) -> Linear(3->hidden) -> (ResBlock x n_blocks) -> Linear(hidden->1)\n",
    "    τ 링크는 기존 그대로: log τ = b0 + b1*bwc + b2*COMED\n",
    "    \"\"\"\n",
    "    def __init__(self, bw_mean: float=70.0, hidden: int=32, n_blocks: int=2,\n",
    "                 dropout_p: float=0.2, use_input_ln: bool=True):\n",
    "        super().__init__()\n",
    "        self.b0 = nn.Parameter(torch.tensor(math.log(24.0)))\n",
    "        self.b1 = nn.Parameter(torch.tensor(0.0))\n",
    "        self.b2 = nn.Parameter(torch.tensor(0.0))\n",
    "        self.bw_mean = float(bw_mean)\n",
    "\n",
    "        self.in_ln  = nn.LayerNorm(3) if use_input_ln else nn.Identity()\n",
    "        self.fc_in  = nn.Linear(3, hidden)\n",
    "        self.blocks = nn.ModuleList([ResBlock(hidden, p=dropout_p) for _ in range(n_blocks)])\n",
    "        self.head   = nn.Linear(hidden, 1)\n",
    "\n",
    "    def _tau(self, bw: torch.Tensor, comed: torch.Tensor):\n",
    "        bwc = (bw - self.bw_mean) / 10.0\n",
    "        log_tau = self.b0 + self.b1*bwc + self.b2*comed\n",
    "        return torch.exp(log_tau).clamp_min(1.0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def tau_from_cov_np(self, bw_np: np.ndarray, cm_np: np.ndarray, device=None):\n",
    "        device = device or next(self.parameters()).device\n",
    "        bw = torch.tensor(bw_np, dtype=torch.float32, device=device)\n",
    "        cm = torch.tensor(cm_np, dtype=torch.float32, device=device)\n",
    "        return self._tau(bw, cm).detach().cpu().numpy()\n",
    "\n",
    "    def forward_single(self, t: torch.Tensor, dose_t: torch.Tensor, dose_a: torch.Tensor,\n",
    "                       bw: float, comed: float):\n",
    "        # 노출 계산(기존과 동일)\n",
    "        tau = self._tau(torch.tensor(bw, dtype=torch.float32, device=t.device),\n",
    "                        torch.tensor(comed, dtype=torch.float32, device=t.device))\n",
    "        dt = t - dose_t\n",
    "        mask = (dt >= 0).float()\n",
    "        exposure = (dose_a * torch.exp(-dt.clamp_min(0) / tau) * mask).sum()\n",
    "\n",
    "        # 피처 순서도 기존과 동일: [E(t), bwc, COMED]\n",
    "        x = torch.stack([\n",
    "            exposure,\n",
    "            (torch.tensor(bw, dtype=torch.float32, device=t.device) - self.bw_mean) / 10.0,\n",
    "            torch.tensor(comed, dtype=torch.float32, device=t.device)\n",
    "        ])\n",
    "        # 잔차 경로\n",
    "        h = self.in_ln(x)\n",
    "        h = self.fc_in(h)\n",
    "        for blk in self.blocks:\n",
    "            h = blk(h)\n",
    "        logit = self.head(h).squeeze()\n",
    "        return logit\n",
    "\n",
    "\n",
    "# -------- MLP 모델 --------\n",
    "class PDMLPClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    입력 x=[E(t), (BW-mean)/10, COMED] -> [Linear+ReLU(+Dropout)]*n_layers -> Linear(→1)\n",
    "    노출 링크: log(tau) = b0 + b1*((BW-mean)/10) + b2*COMED\n",
    "    \"\"\"\n",
    "    def __init__(self, bw_mean: float=70.0, hidden: int=32, n_layers: int=3, dropout_p: float=0.2):\n",
    "        super().__init__()\n",
    "        self.b0 = nn.Parameter(torch.tensor(math.log(24.0)))\n",
    "        self.b1 = nn.Parameter(torch.tensor(0.0))\n",
    "        self.b2 = nn.Parameter(torch.tensor(0.0))\n",
    "        self.bw_mean = float(bw_mean)\n",
    "\n",
    "        layers = []\n",
    "        in_dim = 3\n",
    "        for i in range(n_layers):\n",
    "            layers += [\n",
    "                nn.Linear(in_dim if i==0 else hidden, hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=dropout_p) if dropout_p and dropout_p > 0 else nn.Identity(),\n",
    "            ]\n",
    "        layers += [nn.Linear(hidden if n_layers>0 else in_dim, 1)]\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def _tau(self, bw: torch.Tensor, comed: torch.Tensor):\n",
    "        bwc = (bw - self.bw_mean) / 10.0\n",
    "        log_tau = self.b0 + self.b1*bwc + self.b2*comed\n",
    "        return torch.exp(log_tau).clamp_min(1.0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def tau_from_cov_np(self, bw_np: np.ndarray, cm_np: np.ndarray, device=None):\n",
    "        device = device or next(self.parameters()).device\n",
    "        bw = torch.tensor(bw_np, dtype=torch.float32, device=device)\n",
    "        cm = torch.tensor(cm_np, dtype=torch.float32, device=device)\n",
    "        return self._tau(bw, cm).detach().cpu().numpy()\n",
    "\n",
    "    def forward_single(self, t: torch.Tensor, dose_t: torch.Tensor, dose_a: torch.Tensor,\n",
    "                       bw: float, comed: float):\n",
    "        tau = self._tau(torch.tensor(bw, dtype=torch.float32, device=t.device),\n",
    "                        torch.tensor(comed, dtype=torch.float32, device=t.device))\n",
    "        dt = t - dose_t\n",
    "        mask = (dt >= 0).float()\n",
    "        exposure = (dose_a * torch.exp(-dt.clamp_min(0) / tau) * mask).sum()\n",
    "        x = torch.stack([\n",
    "            exposure,\n",
    "            (torch.tensor(bw, dtype=torch.float32, device=t.device) - self.bw_mean) / 10.0,\n",
    "            torch.tensor(comed, dtype=torch.float32, device=t.device)\n",
    "        ])\n",
    "        return self.mlp(x).squeeze()\n",
    "\n",
    "def _collate(batch): return batch\n",
    "\n",
    "# -------- 지표 계산/도움 함수 --------\n",
    "def _compute_metrics(y_true: np.ndarray, prob: np.ndarray, thr: float = 0.5):\n",
    "    pred = (prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, pred) if len(y_true) else float(\"nan\")\n",
    "    prec = precision_score(y_true, pred, zero_division=0)\n",
    "    rec  = recall_score(y_true, pred, zero_division=0)\n",
    "    f1   = f1_score(y_true, pred, zero_division=0)\n",
    "    try:\n",
    "        roc = roc_auc_score(y_true, prob) if (len(np.unique(y_true))>1) else float(\"nan\")\n",
    "    except Exception:\n",
    "        roc = float(\"nan\")\n",
    "    try:\n",
    "        ap  = average_precision_score(y_true, prob) if (len(np.unique(y_true))>1) else float(\"nan\")\n",
    "    except Exception:\n",
    "        ap  = float(\"nan\")\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, pred, labels=[0,1]).ravel()\n",
    "    except Exception:\n",
    "        tn=fp=fn=tp = 0\n",
    "    return {\"acc\":acc, \"prec\":prec, \"rec\":rec, \"f1\":f1, \"roc_auc\":roc, \"pr_auc\":ap,\n",
    "            \"tn\":tn, \"fp\":fp, \"fn\":fn, \"tp\":tp}\n",
    "\n",
    "def _predict_dataset(model, dataset, batch_size=512):\n",
    "    device = next(model.parameters()).device\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=_collate)\n",
    "    ys, ps = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            for b in batch:\n",
    "                logit = model.forward_single(\n",
    "                    b[\"t\"].to(device), b[\"dose_t\"].to(device), b[\"dose_a\"].to(device),\n",
    "                    float(b[\"bw\"]), float(b[\"cm\"])\n",
    "                )\n",
    "                ys.append(float(b[\"y\"]))\n",
    "                ps.append(torch.sigmoid(logit).item())\n",
    "    return np.array(ys), np.array(ps)\n",
    "\n",
    "def find_best_threshold(y, p, grid=None, metric=\"f1\"):\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.05, 0.95, 91)  # 0.05~0.95\n",
    "    best_thr, best_val = 0.5, -1.0\n",
    "    for thr in grid:\n",
    "        m = _compute_metrics(y, p, thr=thr)\n",
    "        val = m[\"f1\"] if metric==\"f1\" else (0.5*m[\"prec\"] + 0.5*m[\"rec\"])\n",
    "        if val > best_val:\n",
    "            best_val, best_thr = val, thr\n",
    "    return best_thr, best_val\n",
    "\n",
    "# -------- 학습 루틴(베스트 스냅샷/ES/스케줄러/WD) --------\n",
    "def train_classifier(train_ds, valid_ds,\n",
    "                     epochs=60, lr=5e-2, seed=42,\n",
    "                     mlp_hidden=32, mlp_layers=3, mlp_dropout=0.2,\n",
    "                     weight_decay=0.0, patience=10,\n",
    "                     sched_factor=0.5, sched_patience=5,\n",
    "                     clip_norm=None):\n",
    "    device = DEVICE\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    tr_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=_collate, generator=g)\n",
    "    va_loader = DataLoader(valid_ds, batch_size=256, shuffle=False, collate_fn=_collate)\n",
    "\n",
    "    bw_mean = float(np.mean([b[\"bw\"].item() for b in train_ds]))\n",
    "\n",
    "    \n",
    "    if USE_RESIDUAL:\n",
    "            model = PDMLPClassifierResidual(\n",
    "                bw_mean=bw_mean,\n",
    "                hidden=mlp_hidden,\n",
    "                n_blocks=RES_BLOCKS,        # 잔차 “층 수”\n",
    "                dropout_p=mlp_dropout,\n",
    "                use_input_ln=INPUT_LN\n",
    "            ).to(device)\n",
    "    else:\n",
    "            model = PDMLPClassifier(\n",
    "                bw_mean=bw_mean,\n",
    "                hidden=mlp_hidden,\n",
    "                n_layers=mlp_layers,        # 기존 순차형 “층 수”\n",
    "                dropout_p=mlp_dropout\n",
    "            ).to(device)\n",
    "\n",
    "    \n",
    "    model = PDMLPClassifier(bw_mean=bw_mean, hidden=mlp_hidden, n_layers=mlp_layers, dropout_p=mlp_dropout).to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    # ReduceLROnPlateau: 검증 F1 기준, 상승 없으면 LR 감소\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt, mode='max', factor=sched_factor, patience=sched_patience, verbose=True\n",
    "    )\n",
    "\n",
    "    best = (-1.0, None)\n",
    "    best_state = None\n",
    "    es_counter = 0  # EarlyStopping 카운터\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for batch in tr_loader:\n",
    "            opt.zero_grad()\n",
    "            loss = 0.0\n",
    "            for b in batch:\n",
    "                logit = model.forward_single(\n",
    "                    b[\"t\"].to(device), b[\"dose_t\"].to(device), b[\"dose_a\"].to(device),\n",
    "                    float(b[\"bw\"]), float(b[\"cm\"])\n",
    "                )\n",
    "                loss = loss + loss_fn(logit.view(()), b[\"y\"].to(device).view(()))\n",
    "            loss = loss/len(batch)\n",
    "            loss.backward()\n",
    "            if clip_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "            opt.step()\n",
    "\n",
    "        # ---- validation ----\n",
    "        y, p = _predict_dataset(model, valid_ds, batch_size=256)\n",
    "        # 모니터링 지표: F1(thr=0.5). (임계값 튜닝은 학습 후 별도)\n",
    "        metrics = _compute_metrics(y, p, thr=0.5)\n",
    "\n",
    "        # 스케줄러 스텝(F1 기준)\n",
    "        scheduler.step(metrics[\"f1\"])\n",
    "\n",
    "        # 베스트 갱신/스냅샷\n",
    "        if metrics[\"f1\"] > best[0]:\n",
    "            best = (metrics[\"f1\"], {\"epoch\":ep, **metrics})\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            es_counter = 0\n",
    "        else:\n",
    "            es_counter += 1\n",
    "\n",
    "        if ep%10==0 or ep<=3:\n",
    "            lr_cur = opt.param_groups[0][\"lr\"]\n",
    "            print(f\"[ep {ep:03d}] lr={lr_cur:.5f} acc={metrics['acc']:.3f} f1={metrics['f1']:.3f} \"\n",
    "                  f\"prec={metrics['prec']:.3f} rec={metrics['rec']:.3f} \"\n",
    "                  f\"roc_auc={metrics['roc_auc']:.3f} pr_auc={metrics['pr_auc']:.3f}\")\n",
    "\n",
    "        # EarlyStopping\n",
    "        if es_counter >= patience:\n",
    "            print(f\"EarlyStopping at epoch {ep} (no F1 improvement for {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "    # 베스트로 복원\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    print(\"best(valid @thr=0.5):\", best[1])\n",
    "\n",
    "    # ---- 검증셋에서 임계값 최적화 (베스트 모델로) ----\n",
    "    y_val, p_val = _predict_dataset(model, valid_ds, batch_size=256)\n",
    "    best_thr, best_f1 = find_best_threshold(y_val, p_val, grid=None, metric=\"f1\")\n",
    "    tuned_metrics = _compute_metrics(y_val, p_val, thr=best_thr)\n",
    "    print(f\"tuned threshold on valid: thr={best_thr:.3f} | F1={tuned_metrics['f1']:.4f} \"\n",
    "          f\"(prec={tuned_metrics['prec']:.4f}, rec={tuned_metrics['rec']:.4f})\")\n",
    "\n",
    "    return model, best[1], best_thr\n",
    "\n",
    "# -------- 인구 공변량 샘플러 --------\n",
    "def subject_cov_sampler(pd_df: pd.DataFrame, col_BW: Optional[str], col_COMED: Optional[str],\n",
    "                        n:int, scenario:str=\"base\", rng_seed:int=123):\n",
    "    obs_bw = pd_df[col_BW].dropna().to_numpy(dtype=float) if col_BW is not None else np.full(len(pd_df), 80.0)\n",
    "    obs_cm = pd_df[col_COMED].dropna().to_numpy(dtype=float) if col_COMED is not None else np.zeros(len(pd_df))\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    if scenario==\"base\":\n",
    "        idx = rng.integers(0, len(obs_bw), size=n)\n",
    "        return obs_bw[idx], obs_cm[idx]\n",
    "    elif scenario==\"bw_wide\":\n",
    "        bw = rng.uniform(70.0, 140.0, size=n)   # 필요시 관찰 범위로 조정 가능\n",
    "        cm = obs_cm[rng.integers(0, len(obs_cm), size=n)] if len(obs_cm)>0 else np.zeros(n)\n",
    "        return bw, cm\n",
    "    elif scenario==\"no_comed\":\n",
    "        idx = rng.integers(0, len(obs_bw), size=n)\n",
    "        return obs_bw[idx], np.zeros(n)\n",
    "    else:\n",
    "        raise ValueError(\"unknown scenario\")\n",
    "\n",
    "# -------- 정상상태(SS) 평가: MLP --------\n",
    "@torch.no_grad()\n",
    "def success_fraction_for_dose_ss_mlp(model: PDMLPClassifier, dose_mg: float, freq_h: int,\n",
    "                                     last_window_h: int, pd_df, col_BW: Optional[str], col_COMED: Optional[str],\n",
    "                                     Nsubj=300, scenario=\"base\", decision_threshold=0.5,\n",
    "                                     grid_step_h=1.0) -> float:\n",
    "    bw_arr, cm_arr = subject_cov_sampler(pd_df, col_BW, col_COMED, Nsubj, scenario=scenario)\n",
    "    tau = model.tau_from_cov_np(bw_arr, cm_arr, device=next(model.parameters()).device)\n",
    "    tgrid = np.arange(0.0, last_window_h + 1e-6, grid_step_h, dtype=float)\n",
    "    ok = 0\n",
    "    for i in range(Nsubj):\n",
    "        denom = (1.0 - np.exp(-float(freq_h) / max(tau[i],1e-6)))\n",
    "        denom = max(denom, 1e-6)\n",
    "        e_t = dose_mg * np.exp(-tgrid / max(tau[i],1e-6)) / denom\n",
    "        bwc = (bw_arr[i] - model.bw_mean) / 10.0\n",
    "        cm  = cm_arr[i]\n",
    "        X = torch.tensor(np.stack([e_t, np.full_like(e_t, bwc), np.full_like(e_t, cm)], axis=1),\n",
    "                         dtype=torch.float32, device=next(model.parameters()).device)\n",
    "        logits = model.mlp(X).squeeze(1)\n",
    "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        # 윈도우 전체 시간에서 성공 확률이 임계 이상이면 성공(아주 보수적 기준)\n",
    "        if probs.min() >= decision_threshold:\n",
    "            ok += 1\n",
    "    return ok / Nsubj\n",
    "\n",
    "def search_min_dose_ss(model: PDMLPClassifier, grid, freq_h, last_window_h, pd_df,\n",
    "                       col_BW: Optional[str], col_COMED: Optional[str],\n",
    "                       Nsubj=300, scenario=\"base\", target=0.9, decision_threshold=0.5):\n",
    "    rows = []\n",
    "    for d in grid:\n",
    "        frac = success_fraction_for_dose_ss_mlp(model, d, freq_h, last_window_h, pd_df, col_BW, col_COMED,\n",
    "                                                Nsubj=Nsubj, scenario=scenario, decision_threshold=decision_threshold)\n",
    "        rows.append({\"dose\": d, \"fraction\": frac})\n",
    "    df_res = pd.DataFrame(rows).sort_values(\"dose\")\n",
    "    feas = df_res[df_res[\"fraction\"]>=target]\n",
    "    best = feas.iloc[0][\"dose\"] if len(feas)>0 else None\n",
    "    return df_res, best\n",
    "\n",
    "# -------- 학습 실행 (개선 루틴) --------\n",
    "model, valid_best, tuned_thr = train_classifier(\n",
    "    train_ds, valid_ds,\n",
    "    epochs=EPOCHS, lr=LR,\n",
    "    mlp_hidden=MLP_HIDDEN, mlp_layers=MLP_LAYERS, mlp_dropout=MLP_DROPOUT,\n",
    "    weight_decay=WEIGHT_DECAY, patience=PATIENCE,\n",
    "    sched_factor=SCHED_FACTOR, sched_patience=SCHED_PATIENCE,\n",
    "    clip_norm=CLIP_NORM\n",
    ")\n",
    "print(\"\\nValidation best @thr=0.5:\", valid_best)\n",
    "print(f\"Validated tuned decision threshold: {tuned_thr:.3f}\")\n",
    "\n",
    "# -------- 용량 탐색 (튜닝 임계값 적용) --------\n",
    "daily_grid  = [0.5*i for i in range(0, 121)]   # 0..60 mg, 0.5 mg\n",
    "weekly_grid = [5*i   for i in range(0, 41)]    # 0..200 mg, 5 mg\n",
    "\n",
    "daily_base,  best_daily_base  = search_min_dose_ss(model, daily_grid,  24, 24,  pdf, col_BW, col_COMED,\n",
    "                                                   Nsubj=N_SUBJ, scenario=\"base\",\n",
    "                                                   target=0.90, decision_threshold=tuned_thr)\n",
    "weekly_base, best_weekly_base = search_min_dose_ss(model, weekly_grid, 168, 168, pdf, col_BW, col_COMED,\n",
    "                                                   Nsubj=N_SUBJ, scenario=\"base\",\n",
    "                                                   target=0.90, decision_threshold=tuned_thr)\n",
    "\n",
    "daily_bw,   best_daily_bw   = search_min_dose_ss(model, daily_grid,  24, 24,  pdf, col_BW, col_COMED,\n",
    "                                                 Nsubj=N_SUBJ, scenario=\"bw_wide\",\n",
    "                                                 target=0.90, decision_threshold=tuned_thr)\n",
    "weekly_bw,  best_weekly_bw  = search_min_dose_ss(model, weekly_grid, 168, 168, pdf, col_BW, col_COMED,\n",
    "                                                 Nsubj=N_SUBJ, scenario=\"bw_wide\",\n",
    "                                                 target=0.90, decision_threshold=tuned_thr)\n",
    "\n",
    "daily_nocm, best_daily_nocm = search_min_dose_ss(model, daily_grid,  24, 24,  pdf, col_BW, col_COMED,\n",
    "                                                 Nsubj=N_SUBJ, scenario=\"no_comed\",\n",
    "                                                 target=0.90, decision_threshold=tuned_thr)\n",
    "weekly_nocm,best_weekly_nocm= search_min_dose_ss(model, weekly_grid, 168, 168, pdf, col_BW, col_COMED,\n",
    "                                                 Nsubj=N_SUBJ, scenario=\"no_comed\",\n",
    "                                                 target=0.90, decision_threshold=tuned_thr)\n",
    "\n",
    "daily_75,   best_daily_75   = search_min_dose_ss(model, daily_grid,  24, 24,  pdf, col_BW, col_COMED,\n",
    "                                                 Nsubj=N_SUBJ, scenario=\"base\",\n",
    "                                                 target=0.75, decision_threshold=tuned_thr)\n",
    "weekly_75,  best_weekly_75  = search_min_dose_ss(model, weekly_grid, 168, 168, pdf, col_BW, col_COMED,\n",
    "                                                 Nsubj=N_SUBJ, scenario=\"base\",\n",
    "                                                 target=0.75, decision_threshold=tuned_thr)\n",
    "\n",
    "summary = pd.DataFrame([\n",
    "    {\"scenario\":\"Base (Phase 1-like)\", \"target\":\"90%\", \"once-daily (mg)\": best_daily_base,  \"once-weekly (mg)\": best_weekly_base},\n",
    "    {\"scenario\":\"BW 70–140 kg\",        \"target\":\"90%\", \"once-daily (mg)\": best_daily_bw,    \"once-weekly (mg)\": best_weekly_bw},\n",
    "    {\"scenario\":\"No COMED allowed\",    \"target\":\"90%\", \"once-daily (mg)\": best_daily_nocm,  \"once-weekly (mg)\": best_weekly_nocm},\n",
    "    {\"scenario\":\"Base (Phase 1-like)\", \"target\":\"75%\", \"once-daily (mg)\": best_daily_75,    \"once-weekly (mg)\": best_weekly_75},\n",
    "])\n",
    "print(\"\\n=== Dose recommendations summary (thr tuned) ===\")\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "# -------- 테스트 평가 (튜닝 임계값 적용) --------\n",
    "def evaluate_dataset(model, dataset, threshold=0.5, batch_size=512):\n",
    "    y, p = _predict_dataset(model, dataset, batch_size=batch_size)\n",
    "    return _compute_metrics(y, p, thr=threshold)\n",
    "\n",
    "test_metrics = evaluate_dataset(model, test_ds, threshold=tuned_thr)\n",
    "print(\"\\n=== Final TEST metrics (unseen IDs, tuned thr) ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"{k:>8s}: {v:.4f}\")\n",
    "    else:\n",
    "        print(f\"{k:>8s}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f92d5-b2ce-45ab-a446-76e165b0697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "전체 데이터셋 분포같은거 체크해서 test set 만들때 train set이랑 다른 분포 많이 섞어서 test하면 좋을듯~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed4290-026e-4143-b5fd-eff58d633a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff2a0c-148c-4ae8-aec8-a5c1095a0d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gold)",
   "language": "python",
   "name": "gold"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
